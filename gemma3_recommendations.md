# Рекомендации для Gemma-3-12b-it

## Родная конфигурация модели
- **torch_dtype**: `bfloat16` (не float16!)
- **Архитектура**: Gemma3ForConditionalGeneration
- **Размер**: 12B параметров
- **Контекст**: 128K токенов

## Рекомендуемые команды запуска

### 1. С родным bfloat16 (рекомендуется попробовать первым):
```bash
./start.sh --dtype bfloat16 --stream
```

### 2. С 8-битной квантизацией:
```bash
./start.sh --dtype int8 --stream
```

### 3. С 4-битной квантизацией (после наших исправлений):
```bash
./start.sh --dtype int4 --stream
```

## Что мы исправили:
1. Модель теперь автоматически определяет родной dtype из config.json
2. Для квантизации используется bfloat16 как compute dtype (если GPU поддерживает)
3. Убраны token_type_ids, которые Gemma не использует
4. Добавлены специальные параметры генерации для квантизованных моделей

## Почему модель генерировала бред:
Gemma-3 была обучена с bfloat16, а мы использовали float16 для квантизации. Это приводило к потере точности и некорректной генерации.

## Требования к GPU:
- Для bfloat16 нужна поддержка bf16 (RTX 30xx, A100 и новее)
- Для старых GPU придется использовать float16 или квантизацию