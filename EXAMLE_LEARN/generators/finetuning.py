"""
Fine-tuning script generator
"""

import platform
import json
from pathlib import Path
from ..ui.colors import print_progress, print_success, print_error


def create_finetuning_script(installer) -> bool:
    """Create the smart fine-tuning script"""
    print_progress("Creating fine-tuning script...")
    
    script_content = '''#!/usr/bin/env python3
"""
Smart Fine-Tuning Script with Auto-Stop
Automatically trains LLM with intelligent stopping when performance degrades
Supports organized dataset subfolders by format type
Generated by LLM Installer
"""

import os
import sys
import json
import time
import torch
import logging
import argparse
import hashlib
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import matplotlib.pyplot as plt
from dataclasses import dataclass
from collections import defaultdict

# Core ML libraries
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    TrainingArguments, Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from datasets import Dataset
import numpy as np
import warnings
import psutil

# Suppress some warnings
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")
warnings.filterwarnings("ignore", category=UserWarning, module="torch")

@dataclass
class SimpleTrainingConfig:
    """Simple configuration for non-specialists"""
    
    # Model settings (SIMPLE)
    learning_speed: str = "medium"  # slow, medium, fast
    training_intensity: str = "light"  # light, medium, heavy
    max_training_time_minutes: int = 0  # Auto-stop after this time (0 = unlimited)
    
    # Data settings
    data_files: str = "*.json"  # File pattern to load
    data_folders: Optional[List[str]] = None  # Specific subfolders to load from
    max_examples: int = 0  # Limit for faster training (0 = unlimited)
    
    # Quality control (AUTOMATIC)
    patience_bad_results: int = 3  # Stop after 3 bad evaluations
    min_improvement_threshold: float = 0.01  # Minimum improvement to continue
    overfitting_threshold: float = 0.1  # Stop if validation much worse than training
    
    # Output
    save_best_only: bool = True
    
    # Test settings
    test_prompt: str = "Hello, how are you?"  # Test prompt after training
    skip_test: bool = False  # Skip final model test
    
    # Training settings
    eval_steps: int = 50  # Evaluate every N steps
    use_padding: bool = True  # Enable padding for batched processing
    
    # Advanced settings (from command line)
    gradient_accumulation_steps: int = 1  # Number of gradient accumulation steps
    scheduler_type: str = "cosine"  # Learning rate scheduler type
    mixed_precision: str = "no"  # Mixed precision training (no, fp16, bf16)
    use_8bit: bool = False  # Use 8-bit quantization
    use_4bit: bool = False  # Use 4-bit quantization
    gradient_checkpointing: bool = False  # Enable gradient checkpointing
    optim: str = "adamw_torch"  # Optimizer type
    stream_data: bool = False  # Stream data from disk
    checkpoint_steps: int = 0  # Save checkpoint every N steps (0 = disabled)
    warmup_ratio: float = 0.1  # Warmup ratio for learning rate
    
    # Circular training settings
    circular_training: bool = False  # Enable circular training through dataset
    max_circular_epochs: int = 100  # Maximum epochs for circular training
    early_stop_on_perfect: bool = True  # Stop if loss approaches zero


# Paths configured during installation
BASE_DIR = Path(__file__).parent.parent
MODEL_PATH = BASE_DIR / "models" / "base_model"
DATA_PATH = BASE_DIR / "data"
OUTPUT_PATH = BASE_DIR / "checkpoints" / "finetuned_model"
LOG_PATH = BASE_DIR / "logs" / "training.log"

# Ensure directories exist
DATA_PATH.mkdir(exist_ok=True)
OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
LOG_PATH.parent.mkdir(exist_ok=True)


def load_checkpoint_config(checkpoint_path: Path) -> Optional[dict]:
    """Load configuration from checkpoint"""
    # First try training_state.json (in checkpoints)
    state_file = checkpoint_path / "training_state.json"
    if state_file.exists():
        try:
            with open(state_file, 'r') as f:
                state_data = json.load(f)
                return state_data.get('config', {})
        except Exception as e:
            print(f"‚ö†Ô∏è Could not load checkpoint config: {e}")
    
    # Then try training_config.json (in final model)
    config_file = checkpoint_path / "training_config.json"
    if config_file.exists():
        try:
            with open(config_file, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è Could not load training config: {e}")
    
    return None


def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="Smart Fine-Tuning with Auto-Stop",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage (uses all JSON files in data/ and subfolders)
  python finetune.py
  
  # Load data from specific subfolders only
  python finetune.py --data-folders alpaca chatml
  
  # Fast training with limited examples
  python finetune.py --learning-speed fast --max-examples 1000
  
  # Heavy training for maximum quality
  python finetune.py --training-intensity heavy --max-time 60
  
  # Custom data files from specific folders
  python finetune.py --data-files "*.json" --data-folders stackoverflow qa
  
  # Circular training until overfitting
  python finetune.py --circular-training --training-intensity heavy
  
  # Circular training with early stop on perfect fit
  python finetune.py --circular-training --early-stop-on-perfect
  
  # Resume training from last checkpoint
  python finetune.py --resume
  
  # Resume from specific checkpoint
  python finetune.py --resume --checkpoint-path checkpoints/finetuned_model/checkpoint-500

Data Organization:
  You can organize your training data in subfolders by format type:
  
  data/
  ‚îú‚îÄ‚îÄ alpaca/          # instruction/output format
  ‚îú‚îÄ‚îÄ chatml/          # System/User/Assistant format
  ‚îú‚îÄ‚îÄ conversations/   # messages format (OpenAI-style)
  ‚îú‚îÄ‚îÄ qa/              # question/answer format
  ‚îú‚îÄ‚îÄ stackoverflow/   # StackOverflow Q&A with scores/tags
  ‚îî‚îÄ‚îÄ prompt_response/ # prompt/response format
  
  The script will automatically detect and report statistics for each format and folder.

Learning Speed Options:
  slow    - Learning rate 1e-5 (safe, slow convergence)
  medium  - Learning rate 5e-5 (balanced, recommended)
  fast    - Learning rate 1e-4 (fast, might be unstable)

Training Intensity Options:
  light   - Batch size 2, LoRA rank 8 (low memory, slow)
  medium  - Batch size 4, LoRA rank 16 (balanced, recommended)
  heavy   - Batch size 8, LoRA rank 32 (high memory, fast)

Advanced Options:
  --mixed-precision fp16/bf16  - Use mixed precision training for faster speed
  --use-4bit                   - Use 4-bit quantization (75% memory reduction)
  --use-8bit                   - Use 8-bit quantization (50% memory reduction)
  --gradient-checkpointing     - Trade compute for memory (slower but saves RAM)
  --scheduler cosine/linear    - Learning rate schedule type
  --gradient-accumulation N    - Accumulate gradients over N steps
  --circular-training          - Keep cycling through dataset until overfitting
  --max-circular-epochs N      - Maximum epochs for circular training (default: 100)
  --early-stop-on-perfect      - Stop when loss approaches zero
        """
    )
    
    # Model settings
    parser.add_argument("--learning-speed", choices=["slow", "medium", "fast"],
                       default="medium",
                       help="Learning speed: slow/medium/fast (default: medium)")
    
    parser.add_argument("--training-intensity", choices=["light", "medium", "heavy"],
                       default="light", 
                       help="Training intensity: light/medium/heavy (default: light)")
    
    parser.add_argument("--max-time", "--max-training-time", dest="max_training_time_minutes",
                       type=int, default=0,
                       help="Maximum training time in minutes (default: 0 = unlimited)")
    
    # Data settings
    parser.add_argument("--data-files", default="*.json",
                       help="Data files pattern (default: *.json)")
    
    parser.add_argument("--data-folders", nargs='+', default=None,
                       help="Specific subfolders to load data from (e.g., alpaca chatml). Default: all subfolders")
    
    parser.add_argument("--max-examples", type=int, default=0,
                       help="Maximum training examples to use (default: 0 = unlimited)")
    
    # Quality control
    parser.add_argument("--patience", "--patience-bad-results", dest="patience_bad_results",
                       type=int, default=3,
                       help="Stop after N bad evaluations (default: 3)")
    
    parser.add_argument("--min-improvement", "--min-improvement-threshold", 
                       dest="min_improvement_threshold",
                       type=float, default=0.01,
                       help="Minimum improvement to continue (default: 0.01)")
    
    parser.add_argument("--overfitting-threshold", type=float, default=0.1,
                       help="Overfitting threshold (default: 0.1)")
    
    parser.add_argument("--eval-steps", type=int, default=50,
                       help="Evaluate every N steps (default: 50)")
    
    # Test settings
    parser.add_argument("--test-prompt", default="Hello, how are you?",
                       help="Test prompt after training")
    
    parser.add_argument("--skip-test", action="store_true",
                       help="Skip final model test")
    
    # Advanced settings
    parser.add_argument("--gradient-accumulation", "--gradient-accumulation-steps",
                       dest="gradient_accumulation_steps",
                       type=int, default=1,
                       help="Gradient accumulation steps (default: 1)")
    
    parser.add_argument("--scheduler", "--scheduler-type", dest="scheduler_type",
                       choices=["linear", "cosine", "constant"],
                       default="cosine",
                       help="Learning rate scheduler (default: cosine)")
    
    parser.add_argument("--mixed-precision", choices=["no", "fp16", "bf16"],
                       default="no",
                       help="Mixed precision training (default: no)")
    
    parser.add_argument("--use-8bit", action="store_true",
                       help="Use 8-bit quantization for training")
    
    parser.add_argument("--use-4bit", action="store_true",
                       help="Use 4-bit quantization for training")
    
    parser.add_argument("--gradient-checkpointing", action="store_true",
                       help="Enable gradient checkpointing to save memory")
    
    parser.add_argument("--optim", default="adamw_torch",
                       help="Optimizer (default: adamw_torch)")
    
    parser.add_argument("--stream-data", action="store_true",
                       help="Stream large datasets to save memory")
    
    parser.add_argument("--checkpoint-steps", type=int, default=100,
                       help="Save checkpoint every N steps (default: 100)")
    
    parser.add_argument("--warmup-ratio", type=float, default=0.1,
                       help="Warmup ratio (default: 0.1)")
    
    # Circular training arguments
    parser.add_argument("--circular-training", action="store_true",
                       help="Enable circular training through dataset until overfitting")
    
    parser.add_argument("--max-circular-epochs", type=int, default=100,
                       help="Maximum epochs for circular training (default: 100)")
    
    parser.add_argument("--early-stop-on-perfect", action="store_true",
                       help="Stop training if loss approaches zero")
    
    # Resume training arguments
    parser.add_argument("--resume", "--resume-from-checkpoint", dest="resume_from_checkpoint",
                       action="store_true",
                       help="Resume training from the last checkpoint")
    
    parser.add_argument("--checkpoint-path", type=str, default=None,
                       help="Specific checkpoint path to resume from (default: auto-detect latest)")
    
    # Additional flags
    parser.add_argument("--silent", action="store_true",
                       help="Run without confirmation prompts")
    
    args = parser.parse_args()
    
    # Convert args to config
    config = SimpleTrainingConfig(
        learning_speed=args.learning_speed,
        training_intensity=args.training_intensity,
        max_training_time_minutes=args.max_training_time_minutes,
        data_files=args.data_files,
        data_folders=args.data_folders,
        max_examples=args.max_examples,
        patience_bad_results=args.patience_bad_results,
        min_improvement_threshold=args.min_improvement_threshold,
        overfitting_threshold=args.overfitting_threshold,
        test_prompt=args.test_prompt,
        skip_test=args.skip_test,
        eval_steps=args.eval_steps,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        scheduler_type=args.scheduler_type,
        mixed_precision=args.mixed_precision,
        use_8bit=args.use_8bit,
        use_4bit=args.use_4bit,
        gradient_checkpointing=args.gradient_checkpointing,
        optim=args.optim,
        stream_data=args.stream_data,
        checkpoint_steps=args.checkpoint_steps,
        warmup_ratio=args.warmup_ratio,
        circular_training=args.circular_training,
        max_circular_epochs=args.max_circular_epochs,
        early_stop_on_perfect=args.early_stop_on_perfect
    )
    
    # Add resume parameters to args for later use
    args.resume_from_checkpoint = args.resume_from_checkpoint
    args.checkpoint_path = args.checkpoint_path
    
    return config, args


class SmartTrainer:
    def __init__(self, config: SimpleTrainingConfig):
        self.config = config
        
        # Setup logging
        self.setup_logging()
        
        # Load model info if available
        self.model_info = self.load_model_info()
        
        # Determine device with memory check
        self.device = self._get_best_device()
        
        self.best_loss = float('inf')
        self.bad_epochs = 0
        self.training_losses = []
        self.validation_losses = []
        self.learning_rates_history = []
        self.memory_usage_history = []
        self.start_time = None
        self._current_train_loss = float('inf')
        self.min_evaluations_before_overfitting_check = 5  # Don't check overfitting too early
        
        # Convert simple parameters to technical ones
        self.learning_rates = {"slow": 1e-5, "medium": 5e-5, "fast": 1e-4}
        self.batch_sizes = {"light": 2, "medium": 4, "heavy": 8}
        self.lora_ranks = {"light": 8, "medium": 16, "heavy": 32}
        
        print(f"üöÄ Smart Trainer initialized")
        print(f"üì± Device: {self.device}")
        print(f"ü§ñ Model: {MODEL_PATH}")
        print(f"üìä Data: {DATA_PATH}")
        print(f"üíæ Output: {OUTPUT_PATH}")
        print(f"‚ö° Learning Speed: {self.config.learning_speed}")
        print(f"üí™ Training Intensity: {self.config.training_intensity}")
        
        # Show circular training info
        if self.config.circular_training:
            print(f"üîÑ Circular Training: Enabled (max {self.config.max_circular_epochs} epochs)")
            print(f"üõë Early Stop on Perfect: {'Yes' if self.config.early_stop_on_perfect else 'No'}")
        
        # Show model info if available
        if self.model_info:
            if 'model_config' in self.model_info:
                model_config = self.model_info['model_config']
                if 'max_position_embeddings' in model_config:
                    print(f"üìè Max context: {model_config['max_position_embeddings']:,} tokens")
                if 'use_flash_attention_2' in model_config:
                    print(f"‚ö° Flash Attention 2: {'Enabled' if model_config['use_flash_attention_2'] else 'Disabled'}")
    
    def _get_torch_dtype(self):
        """Get torch dtype based on configuration and device"""
        # Check if we have saved torch dtype preference
        if hasattr(self, 'model_info') and self.model_info and 'torch_dtype' in self.model_info:
            dtype_str = self.model_info['torch_dtype']
            dtype_map = {
                "bfloat16": torch.bfloat16,
                "float16": torch.float16,
                "float32": torch.float32
            }
            if dtype_str in dtype_map:
                return dtype_map[dtype_str]
        
        # Auto-detect based on device
        if self.device == "cuda":
            if torch.cuda.is_bf16_supported():
                return torch.bfloat16
            return torch.float16
        elif self.device == "mps":
            return torch.float16
        else:
            return torch.float32
    
    def _get_memory_usage(self):
        """Get current memory usage in GB"""
        if self.device == "cuda":
            return torch.cuda.memory_allocated() / 1024**3
        else:
            process = psutil.Process()
            return process.memory_info().rss / 1024**3
    
    def _get_total_memory(self):
        """Get total memory in GB"""
        if self.device == "cuda":
            return torch.cuda.get_device_properties(0).total_memory / 1024**3
        else:
            return psutil.virtual_memory().total / 1024**3
    
    def _adjust_batch_size(self, target_batch_size):
        """Adjust batch size based on available memory"""
        if self.device != "cuda":
            # MPS has limitations
            if self.device == "mps":
                return min(target_batch_size, 4)
            return target_batch_size
        
        # Estimate memory requirement
        available_memory = self._get_total_memory() - self._get_memory_usage()
        
        # Rough estimation: each sample needs ~1GB for typical models
        # Adjust based on model size if we have model info
        if hasattr(self, 'model_info') and self.model_info and 'model_config' in self.model_info:
            model_config = self.model_info['model_config']
            model_params = model_config.get('num_hidden_layers', 12) * model_config.get('hidden_size', 768)
        else:
            model_params = 12 * 768  # Default estimation
        
        memory_per_sample = max(0.5, model_params / 1e9)  # GB per sample
        
        max_possible_batch = int(available_memory * 0.8 / memory_per_sample)  # Use 80% of available
        
        adjusted_batch_size = min(target_batch_size, max_possible_batch)
        
        if adjusted_batch_size < target_batch_size:
            print(f"‚ö†Ô∏è Reduced batch size from {target_batch_size} to {adjusted_batch_size} due to memory constraints")
        
        return max(1, adjusted_batch_size)

    def setup_logging(self):
        """Setup logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(LOG_PATH, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_model_info(self) -> Optional[Dict[str, Any]]:
        """Load model info saved during installation"""
        model_info_path = BASE_DIR / "model_info.json"
        if model_info_path.exists():
            try:
                with open(model_info_path, 'r') as f:
                    return json.load(f)
            except Exception as e:
                self.logger.warning(f"Could not load model info: {e}")
        return None
    
    def _get_best_device(self) -> str:
        """Get the best available device with memory check"""
        if torch.cuda.is_available():
            # Select GPU with most free memory if multiple GPUs
            if torch.cuda.device_count() > 1:
                max_free_mem = 0
                best_gpu = 0
                for i in range(torch.cuda.device_count()):
                    free_mem, _ = torch.cuda.mem_get_info(i)
                    if free_mem > max_free_mem:
                        max_free_mem = free_mem
                        best_gpu = i
                torch.cuda.set_device(best_gpu)
                print(f"üéØ Selected GPU {best_gpu} with {max_free_mem / 1e9:.1f}GB free memory")
            return "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"

    def load_and_prepare_data(self) -> Tuple[Dataset, Dataset]:
        """Load JSON data and prepare for training with deduplication and subfolder support"""
        print("üìä Loading training data...")
        
        # Ensure data directory exists
        if not DATA_PATH.exists():
            DATA_PATH.mkdir(parents=True, exist_ok=True)
            print(f"üìÅ Created data directory: {DATA_PATH}")
        
        # Determine which folders to search
        search_folders = []
        
        # If specific folders requested, use those
        if self.config.data_folders:
            for folder_name in self.config.data_folders:
                folder_path = DATA_PATH / folder_name
                if folder_path.exists() and folder_path.is_dir():
                    search_folders.append(folder_path)
                else:
                    print(f"‚ö†Ô∏è Folder not found: {folder_path}")
        else:
            # Otherwise, search all subfolders + root
            search_folders = [DATA_PATH]
            for subfolder in DATA_PATH.iterdir():
                if subfolder.is_dir() and not subfolder.name.startswith('.'):
                    search_folders.append(subfolder)
        
        # Collect all data files from selected folders
        all_files = []
        folder_file_counts = defaultdict(int)
        
        for folder in search_folders:
            folder_files = list(folder.glob(self.config.data_files))
            all_files.extend(folder_files)
            folder_file_counts[folder.name if folder != DATA_PATH else "root"] = len(folder_files)
        
        # Show folder statistics
        if folder_file_counts:
            print(f"üìÇ Data source folders:")
            for folder_name, count in sorted(folder_file_counts.items()):
                if count > 0:
                    print(f"   - {folder_name}: {count} files")
        
        print(f"üîé Total files found: {len(all_files)}")
        
        if not all_files:
            raise FileNotFoundError(f"""
‚ùå No files found matching pattern '{self.config.data_files}'

Searched in: {', '.join(str(f) for f in search_folders)}

To add training data:
1. Create organized subfolders in {DATA_PATH}:
   - alpaca/     # For instruction/output format
   - chatml/     # For System/User/Assistant format  
   - conversations/  # For messages format
   - qa/         # For question/answer format
   - stackoverflow/  # For StackOverflow Q&A format
   - prompt_response/  # For prompt/response format

2. Create JSON files in one of these formats:
   - {{"System": "system prompt", "User": "question", "Assistant": "answer"}}
   - {{"instruction": "question", "output": "answer"}}
   - {{"prompt": "question", "response": "answer"}}
   - {{"question": "question", "answer": "answer"}}
   - {{"messages": [{{"role": "user", "content": "question"}}, {{"role": "assistant", "content": "answer"}}]}}
   - {{"question": "Q title", "answer": "answer text", "score": 42, "tags": ["python", "ml"], "accepted": true}}

3. Run this script again (optionally with --data-folders to select specific folders)
""")
        
        # Load all JSON data with folder tracking
        all_data = []
        folder_data_counts = defaultdict(int)
        format_folder_mapping = defaultdict(lambda: defaultdict(int))
        
        for file_path in all_files:
            folder_name = file_path.parent.name if file_path.parent != DATA_PATH else "root"
            print(f"   üîÑ Processing {file_path.relative_to(DATA_PATH)}")
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        for item in data:
                            item['_source_folder'] = folder_name
                            item['_source_file'] = file_path.name
                        all_data.extend(data)
                        folder_data_counts[folder_name] += len(data)
                    else:
                        data['_source_folder'] = folder_name
                        data['_source_file'] = file_path.name
                        all_data.append(data)
                        folder_data_counts[folder_name] += 1
            except Exception as e:
                print(f"   ‚ùå Error loading {file_path}: {e}")
                continue
        
        if not all_data:
            raise ValueError("No valid data loaded from any files!")
        
        print(f"‚úÖ Loaded {len(all_data)} total examples")
        
        # Show per-folder statistics
        if folder_data_counts:
            print(f"üìä Examples per folder:")
            for folder_name, count in sorted(folder_data_counts.items()):
                print(f"   - {folder_name}: {count} examples")
        
        # Deduplication
        print("üîÑ Removing duplicates...")
        unique_data = []
        seen_hashes = set()
        
        for item in all_data:
            # Create a copy without metadata for hashing
            item_copy = {k: v for k, v in item.items() if not k.startswith('_')}
            item_str = json.dumps(item_copy, sort_keys=True, ensure_ascii=False)
            item_hash = hashlib.md5(item_str.encode('utf-8')).hexdigest()
            
            if item_hash not in seen_hashes:
                seen_hashes.add(item_hash)
                unique_data.append(item)
        
        duplicates_removed = len(all_data) - len(unique_data)
        if duplicates_removed > 0:
            print(f"‚úÖ Removed {duplicates_removed} duplicate examples")
        
        all_data = unique_data
        print(f"üìä Unique examples: {len(all_data)}")
        
        # Limit examples if requested
        if self.config.max_examples > 0 and len(all_data) > self.config.max_examples:
            all_data = all_data[:self.config.max_examples]
            print(f"üîÑ Limited to {self.config.max_examples} examples")
        
        # Convert to training format
        processed_data = []
        format_counts = {
            'System/User/Assistant': 0,
            'instruction/output': 0,
            'prompt/response': 0,
            'question/answer': 0,
            'messages': 0,
            'stackoverflow': 0
        }
        
        for item in all_data:
            source_folder = item.get('_source_folder', 'unknown')
            
            # Format 1: System/User/Assistant (ChatML-style)
            if 'System' in item and 'User' in item and 'Assistant' in item:
                text = f"System: {item['System']}\\n\\nUser: {item['User']}\\n\\nAssistant: {item['Assistant']}"
                processed_data.append({"text": text})
                format_counts['System/User/Assistant'] += 1
                format_folder_mapping['System/User/Assistant'][source_folder] += 1
            
            # Format 2: instruction/output (Alpaca-style)
            elif 'instruction' in item and 'output' in item:
                text = f"User: {item['instruction']}"
                
                # Handle input field
                input_value = item.get('input', '')
                if input_value:
                    if isinstance(input_value, dict):
                        input_str = json.dumps(input_value, ensure_ascii=False, indent=2)
                    else:
                        input_str = str(input_value)
                    
                    if input_str.strip():
                        text += f"\\n{input_str}"
                
                text += f"\\n\\nAssistant: {item['output']}"
                processed_data.append({"text": text})
                format_counts['instruction/output'] += 1
                format_folder_mapping['instruction/output'][source_folder] += 1
            
            # Format 3: prompt/response
            elif 'prompt' in item and 'response' in item:
                text = f"User: {item['prompt']}\\n\\nAssistant: {item['response']}"
                processed_data.append({"text": text})
                format_counts['prompt/response'] += 1
                format_folder_mapping['prompt/response'][source_folder] += 1
            
            # Format 4: question/answer
            elif 'question' in item and 'answer' in item:
                # Check if it's StackOverflow format
                if any(key in item for key in ['score', 'tags', 'accepted']):
                    # StackOverflow format
                    text = f"User: {item['question']}"
                    
                    # Add tags if available
                    if 'tags' in item and item['tags']:
                        tags_str = ', '.join(item['tags']) if isinstance(item['tags'], list) else str(item['tags'])
                        text += f"\\nTags: {tags_str}"
                    
                    text += f"\\n\\nAssistant: {item['answer']}"
                    
                    # Add metadata as a comment if it has high score or is accepted
                    if item.get('accepted', False) or item.get('score', 0) > 10:
                        text += f"\\n[Accepted answer]" if item.get('accepted', False) else ""
                        if item.get('score', 0) > 0:
                            text += f" [Score: {item['score']}]"
                    
                    processed_data.append({"text": text})
                    format_counts['stackoverflow'] += 1
                    format_folder_mapping['stackoverflow'][source_folder] += 1
                else:
                    # Regular Q&A format
                    text = f"User: {item['question']}\\n\\nAssistant: {item['answer']}"
                    processed_data.append({"text": text})
                    format_counts['question/answer'] += 1
                    format_folder_mapping['question/answer'][source_folder] += 1
            
            # Format 5: messages format (OpenAI-style)
            elif 'messages' in item and isinstance(item['messages'], list):
                text_parts = []
                for msg in item['messages']:
                    role = msg.get('role', '').capitalize()
                    content = msg.get('content', '')
                    if role and content:
                        text_parts.append(f"{role}: {content}")
                if text_parts:
                    text = "\\n\\n".join(text_parts)
                    processed_data.append({"text": text})
                    format_counts['messages'] += 1
                    format_folder_mapping['messages'][source_folder] += 1
        
        if not processed_data:
            raise ValueError("""No valid training examples found!
            
Supported formats:
1. {"System": "system prompt", "User": "question", "Assistant": "answer"}
2. {"instruction": "question", "output": "answer", "input": "optional context"}
3. {"prompt": "question", "response": "answer"}
4. {"question": "question", "answer": "answer"}
5. {"messages": [{"role": "user", "content": "question"}, {"role": "assistant", "content": "answer"}]}
6. {"question": "Q title", "answer": "answer text", "score": 42, "tags": ["python", "ml"], "accepted": true}
7. Plain text files (.txt) or {"text": "content"} for continued pretraining

Examples:
- StackOverflow format:
  {
      "question": "How to use async/await in Python?",
      "answer": "Python 3.5+ supports async/await syntax...",
      "score": 156,
      "tags": ["python", "async", "asyncio"],
      "accepted": true
  }

- Plain text format (for books, articles, documentation):
  {"text": "Chapter 1: Introduction to Machine Learning..."}
  
  Or simply save as .txt files in data/plaintext/

Please check your files have one of these formats.""")
        
        print(f"‚úÖ Processed {len(processed_data)} valid examples")
        
        # Show detailed format statistics
        detected_formats = [fmt for fmt, count in format_counts.items() if count > 0]
        if detected_formats:
            print(f"üìã Detected formats:")
            for fmt, count in format_counts.items():
                if count > 0:
                    print(f"   - {fmt}: {count} examples")
                    # Show which folders contained this format
                    if fmt in format_folder_mapping:
                        folder_breakdown = format_folder_mapping[fmt]
                        if len(folder_breakdown) > 1 or (len(folder_breakdown) == 1 and 'root' not in folder_breakdown):
                            for folder, folder_count in sorted(folder_breakdown.items()):
                                print(f"      ‚Ä¢ {folder}: {folder_count}")
        
        # Split train/validation (80/20)
        split_idx = int(len(processed_data) * 0.8)
        train_data = processed_data[:split_idx]
        val_data = processed_data[split_idx:]
        
        print(f"üìö Training examples: {len(train_data)}")
        print(f"üîç Validation examples: {len(val_data)}")
        
        return Dataset.from_list(train_data), Dataset.from_list(val_data)

    def find_latest_checkpoint(self) -> Optional[Path]:
        """Find the latest checkpoint in the output directory"""
        checkpoint_pattern = OUTPUT_PATH / "checkpoint-*"
        checkpoints = list(OUTPUT_PATH.glob("checkpoint-*"))
        
        if not checkpoints:
            return None
        
        # Sort by step number
        def get_step_number(checkpoint_path):
            try:
                return int(checkpoint_path.name.split("-")[-1])
            except:
                return -1
        
        checkpoints.sort(key=get_step_number, reverse=True)
        return checkpoints[0]
    
    def setup_model_and_tokenizer(self):
        """Load model and tokenizer with LoRA"""
        print("ü§ñ Loading model and tokenizer...")
        
        if not MODEL_PATH.exists():
            raise FileNotFoundError(f"Model not found at {MODEL_PATH}")
        
        # Check if this is a diffusion model
        model_info = self.load_model_info()
        if model_info and model_info.get('is_diffusion_model', False):
            print("‚ùå This is a diffusion/image generation model!")
            print("   Fine-tuning diffusion models requires specialized training scripts.")
            print("   This script is designed for text generation models only.")
            sys.exit(1)
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            str(MODEL_PATH), 
            trust_remote_code=True
        )
        
        # Add padding token if missing
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        # Determine torch dtype from model info or auto-detect
        torch_dtype = self._determine_torch_dtype()
        
        # Prepare quantization config if needed
        quantization_config = None
        if self.config.use_4bit:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch_dtype,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True
            )
            print("üî¢ Using 4-bit quantization")
        elif self.config.use_8bit:
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True
            )
            print("üî¢ Using 8-bit quantization")
        elif self.model_info and 'quantization_config' in self.model_info:
            # Use saved quantization config if model was installed with it
            saved_quant = self.model_info['quantization_config']
            if saved_quant:
                quantization_config = BitsAndBytesConfig(**saved_quant)
                print("üî¢ Using saved quantization config")
        
        # Model loading configuration
        model_kwargs = {
            "torch_dtype": torch_dtype,
            "trust_remote_code": True,
            "low_cpu_mem_usage": True
        }
        
        # Add device map for CUDA
        if self.device == "cuda":
            model_kwargs["device_map"] = "auto"
        
        # Add quantization config if available
        if quantization_config:
            model_kwargs["quantization_config"] = quantization_config
        
        # Add attention implementation from model config
        if self.model_info and 'model_config' in self.model_info:
            model_config = self.model_info['model_config']
            if model_config.get('use_flash_attention_2') and self.device == "cuda":
                model_kwargs["attn_implementation"] = "flash_attention_2"
                print("‚ö° Enabling Flash Attention 2")
        
        # Load model
        if self.device == "mps":
            # MPS-specific loading
            model_kwargs.pop("device_map", None)  # Remove device_map for MPS
            self.model = AutoModelForCausalLM.from_pretrained(
                str(MODEL_PATH),
                **model_kwargs
            ).to("mps")
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                str(MODEL_PATH),
                **model_kwargs
            )
        
        # Enable gradient checkpointing if requested
        if self.config.gradient_checkpointing:
            self.model.gradient_checkpointing_enable()
            print("üíæ Gradient checkpointing enabled")
        
        # Prepare model for k-bit training if using quantization
        if quantization_config:
            self.model = prepare_model_for_kbit_training(self.model)
        
        # Setup LoRA
        lora_rank = self.lora_ranks[self.config.training_intensity]
        
        # Adjust LoRA rank based on available memory if using heavy intensity
        if self.config.training_intensity == "heavy" and self.device == "cuda":
            free_mem, _ = torch.cuda.mem_get_info()
            if free_mem < 8 * 1e9:  # Less than 8GB free
                lora_rank = min(lora_rank, 16)
                print(f"‚ö†Ô∏è Reduced LoRA rank to {lora_rank} due to memory constraints")
        
        # Determine target modules based on model architecture
        if hasattr(self.model.config, 'model_type'):
            model_type = self.model.config.model_type
        else:
            model_type = 'unknown'
            
        if model_type in ['gpt2', 'gpt-neo', 'gptj']:
            # GPT2-style models use different layer names
            target_modules = ["c_attn", "c_proj", "c_fc"]
        elif model_type in ['llama', 'mistral', 'mixtral', 'qwen2', 'yi', 'vicuna', 'alpaca', 'codellama']:
            # Llama-style transformer models
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
        elif model_type in ['phi', 'phi3']:
            # Phi models
            target_modules = ["q_proj", "k_proj", "v_proj", "dense"]
        elif model_type == 'gemma':
            # Gemma models
            target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
        elif model_type == 'falcon':
            # Falcon models
            target_modules = ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]
        elif model_type == 'stablelm':
            # StableLM models
            target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
        else:
            # Try common patterns as fallback
            print(f"‚ö†Ô∏è Unknown model type '{model_type}', using fallback LoRA configuration")
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "c_attn", "c_proj"]
        
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=lora_rank,
            lora_alpha=lora_rank * 2,
            lora_dropout=0.1,
            target_modules=target_modules
        )
        
        self.model = get_peft_model(self.model, lora_config)
        print(f"‚úÖ Model loaded with LoRA (rank={lora_rank})")
        
        # Print trainable parameters
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"üìä Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)")
        
        # Log memory usage
        if self.device == "cuda":
            allocated = torch.cuda.memory_allocated() / 1e9
            reserved = torch.cuda.memory_reserved() / 1e9
            print(f"üíæ GPU Memory: {allocated:.1f}GB allocated, {reserved:.1f}GB reserved")
    
    def _determine_torch_dtype(self):
        """Determine the best torch dtype to use"""
        # First check if we have a saved preference
        if self.model_info and 'torch_dtype' in self.model_info:
            dtype_str = self.model_info['torch_dtype']
            print(f"üìå Using saved dtype preference: {dtype_str}")
            if dtype_str == "bfloat16":
                return torch.bfloat16
            elif dtype_str == "float16":
                return torch.float16
            elif dtype_str == "float32":
                return torch.float32
        
        # Otherwise auto-detect based on device
        if self.device == "cuda":
            # Check for BF16 support
            if torch.cuda.is_bf16_supported():
                print("üìå Using bfloat16 (GPU supports it)")
                return torch.bfloat16
            else:
                print("üìå Using float16 (GPU doesn't support bfloat16)")
                return torch.float16
        elif self.device == "mps":
            print("üìå Using float16 for MPS")
            return torch.float16
        else:
            print("üìå Using float32 for CPU")
            return torch.float32

    def tokenize_function(self, examples):
        """Tokenize examples"""
        texts = examples["text"]
        if isinstance(texts, str):
            texts = [texts]
        
        model_inputs = self.tokenizer(
            texts,
            truncation=True,
            max_length=512,
            padding=False,
            return_attention_mask=True,
            return_tensors=None
        )
        
        return model_inputs

    def should_stop_training(self, eval_loss: float, train_loss: float) -> Tuple[bool, str]:
        """Intelligent decision to stop training"""
        
        # Check time limit
        if self.config.max_training_time_minutes > 0 and self.start_time:
            if (time.time() - self.start_time) > (self.config.max_training_time_minutes * 60):
                return True, f"‚è∞ Time limit reached ({self.config.max_training_time_minutes} minutes)"
        
        # Check for perfect training (if enabled)
        if self.config.early_stop_on_perfect and train_loss < 0.01:
            return True, f"üéØ Perfect training achieved (loss: {train_loss:.4f})"
        
        # Skip overfitting check if we haven't done enough evaluations
        if len(self.validation_losses) < self.min_evaluations_before_overfitting_check:
            # Too early to check for overfitting
            pass
        # For circular training, be more lenient with overfitting
        elif self.config.circular_training:
            # Only stop on severe overfitting with increasing validation loss
            if len(self.training_losses) > 5 and len(self.validation_losses) > 5:
                recent_train = np.mean(self.training_losses[-5:])
                recent_val = np.mean(self.validation_losses[-5:])
                
                # Check if validation loss is actually increasing (not just higher than train)
                val_trend = np.polyfit(range(5), self.validation_losses[-5:], 1)[0]
                
                # More tolerant threshold for circular training
                overfitting_factor = 2.0
                if recent_val > recent_train + (self.config.overfitting_threshold * overfitting_factor):
                    # Only consider it overfitting if val loss is trending upward
                    if val_trend > 0:
                        # But still check if we're making overall progress
                        if len(self.validation_losses) > 10:
                            early_val = np.mean(self.validation_losses[:5])
                            if recent_val < early_val * 0.8:  # 20% improvement overall
                                return False, "üìà Still improving despite gap between train/val loss"
                        return True, f"üö´ Severe overfitting detected (val loss increasing)"
                    else:
                        # Val loss is still decreasing, so not really overfitting
                        return False, "üìä Val loss higher than train but still decreasing"
        else:
            # Standard overfitting check for non-circular training
            if len(self.training_losses) > 3 and len(self.validation_losses) > 3:
                recent_train = np.mean(self.training_losses[-3:])
                recent_val = np.mean(self.validation_losses[-3:])
                
                # Check validation loss trend
                val_trend = np.polyfit(range(3), self.validation_losses[-3:], 1)[0]
                
                if recent_val > recent_train + self.config.overfitting_threshold:
                    # Only consider it overfitting if val loss is trending upward
                    if val_trend > 0:
                        return True, f"üö´ Overfitting detected (val loss increasing)"
                    else:
                        # Check if the gap is growing
                        if len(self.validation_losses) > 6:
                            old_gap = np.mean(self.validation_losses[-6:-3]) - np.mean(self.training_losses[-6:-3])
                            new_gap = recent_val - recent_train
                            if new_gap > old_gap * 1.5:  # Gap growing by 50%
                                return True, f"üö´ Overfitting detected (growing train/val gap)"
                        return False, "üìä Val loss higher than train but still improving"
        
        # Check for improvement (relaxed for circular training)
        if eval_loss < self.best_loss - self.config.min_improvement_threshold:
            self.best_loss = eval_loss
            self.bad_epochs = 0
            return False, "‚úÖ Model improved"
        else:
            self.bad_epochs += 1
            
            # More patience for circular training
            patience_multiplier = 3 if self.config.circular_training else 1
            if self.bad_epochs >= self.config.patience_bad_results * patience_multiplier:
                return True, f"üõë No improvement for {self.bad_epochs} evaluations"
        
        # Check for loss explosion
        if eval_loss > 10 or (len(self.validation_losses) > 0 and eval_loss > 2 * self.validation_losses[0]):
            return True, f"üí• Loss explosion detected (eval_loss: {eval_loss:.4f})"
        
        # Check memory usage on CUDA
        if self.device == "cuda":
            allocated = torch.cuda.memory_allocated() / 1e9
            reserved = torch.cuda.memory_reserved() / 1e9
            total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
            
            if reserved > 0.95 * total_mem:
                return True, f"üíæ GPU memory exhausted ({reserved:.1f}/{total_mem:.1f}GB)"
        
        return False, "üîÑ Continue training"

    def train(self):
        """Main training function"""
        print("\\n" + "="*60)
        print("üéØ STARTING SMART FINE-TUNING")
        print("="*60)
        
        self.start_time = time.time()
        
        try:
            # Load data
            train_dataset, val_dataset = self.load_and_prepare_data()
            
            # Setup model
            self.setup_model_and_tokenizer()
            
            # Tokenize datasets
            print("üî§ Tokenizing datasets...")
            train_dataset = train_dataset.map(self.tokenize_function, batched=True, remove_columns=["text"])
            val_dataset = val_dataset.map(self.tokenize_function, batched=True, remove_columns=["text"])
            
            # Data collator
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False,
                pad_to_multiple_of=None,
                return_tensors="pt"
            )
            
            # Training arguments
            learning_rate = self.learning_rates[self.config.learning_speed]
            batch_size = self.batch_sizes[self.config.training_intensity]
            
            # Adjust batch size for device and memory
            effective_batch_size = batch_size
            if self.device == "mps" and batch_size > 4:
                batch_size = 4
                print("‚ö†Ô∏è Reduced batch size to 4 for MPS compatibility")
            elif self.device == "cuda":
                # Dynamic batch size adjustment based on available memory
                free_mem, _ = torch.cuda.mem_get_info()
                if free_mem < 4 * 1e9 and batch_size > 2:  # Less than 4GB
                    batch_size = 2
                    print(f"‚ö†Ô∏è Reduced batch size to 2 due to memory ({free_mem/1e9:.1f}GB free)")
                elif free_mem < 8 * 1e9 and batch_size > 4:  # Less than 8GB
                    batch_size = 4
                    print(f"‚ö†Ô∏è Reduced batch size to 4 due to memory ({free_mem/1e9:.1f}GB free)")
            
            # Calculate gradient accumulation to maintain effective batch size
            gradient_accumulation_steps = max(1, effective_batch_size // batch_size)
            if self.config.gradient_accumulation_steps > 1:
                gradient_accumulation_steps = self.config.gradient_accumulation_steps
            
            if gradient_accumulation_steps > 1:
                print(f"üìä Using gradient accumulation: {gradient_accumulation_steps} steps")
                print(f"üì¶ Effective batch size: {batch_size * gradient_accumulation_steps}")
            
            # Determine mixed precision settings
            use_fp16 = False
            use_bf16 = False
            if self.config.mixed_precision != "no" and self.device == "cuda":
                if self.config.mixed_precision == "bf16" and torch.cuda.is_bf16_supported():
                    use_bf16 = True
                    print("üéØ Using BF16 mixed precision training")
                elif self.config.mixed_precision == "fp16":
                    use_fp16 = True
                    print("üéØ Using FP16 mixed precision training")
                elif self.config.mixed_precision == "bf16":
                    # Fallback to FP16 if BF16 not supported
                    use_fp16 = True
                    print("‚ö†Ô∏è BF16 not supported, using FP16 instead")
            
            # Determine number of epochs
            num_epochs = self.config.max_circular_epochs if self.config.circular_training else 10
            
            training_args = TrainingArguments(
                output_dir=str(OUTPUT_PATH),
                num_train_epochs=num_epochs,
                per_device_train_batch_size=batch_size,
                per_device_eval_batch_size=batch_size,
                gradient_accumulation_steps=gradient_accumulation_steps,
                learning_rate=learning_rate,
                warmup_ratio=self.config.warmup_ratio,
                warmup_steps=50,
                logging_steps=10,
                eval_steps=self.config.eval_steps,
                save_steps=self.config.checkpoint_steps,
                eval_strategy="steps",
                save_strategy="steps",
                load_best_model_at_end=True,
                metric_for_best_model="eval_loss",
                greater_is_better=False,
                remove_unused_columns=True,
                dataloader_pin_memory=self.device == "cuda",
                fp16=use_fp16,
                bf16=use_bf16,
                use_mps_device=self.device == "mps",
                gradient_checkpointing=self.config.gradient_checkpointing,
                lr_scheduler_type=self.config.scheduler_type,
                optim=self.config.optim,
                save_total_limit=3,  # Keep only 3 best checkpoints
                report_to=None,
                ddp_find_unused_parameters=False if torch.cuda.device_count() > 1 else None,
            )
            
            # Custom trainer with smart stopping
            class SmartHFTrainer(Trainer):
                def __init__(self, smart_trainer_instance, *args, **kwargs):
                    super().__init__(*args, **kwargs)
                    self.smart_trainer = smart_trainer_instance
                
                def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
                    # Clear GPU cache before evaluation
                    if self.smart_trainer.device == "cuda":
                        torch.cuda.empty_cache()
                    
                    eval_results = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)
                    
                    eval_loss = eval_results.get("eval_loss", float('inf'))
                    
                    # Find the most recent training loss from log history
                    train_loss = float('inf')
                    if self.state.log_history:
                        for log_entry in reversed(self.state.log_history):
                            if "loss" in log_entry and not any(k.startswith("eval_") for k in log_entry):
                                train_loss = log_entry["loss"]
                                break
                    
                    # Fallback: try to get from trainer state
                    if train_loss == float('inf') and hasattr(self.state, 'loss') and self.state.loss is not None:
                        train_loss = self.state.loss
                    
                    # Final fallback: use the stored current train loss
                    if train_loss == float('inf') and hasattr(self.smart_trainer, '_current_train_loss'):
                        train_loss = self.smart_trainer._current_train_loss
                    
                    self.smart_trainer.validation_losses.append(eval_loss)
                    if train_loss != float('inf'):
                        self.smart_trainer.training_losses.append(train_loss)
                    
                    # Track learning rate
                    if hasattr(self.optimizer, 'param_groups'):
                        current_lr = self.optimizer.param_groups[0]['lr']
                        self.smart_trainer.learning_rates_history.append(current_lr)
                    
                    # Track memory usage
                    if self.smart_trainer.device == "cuda":
                        allocated = torch.cuda.memory_allocated() / 1e9
                        self.smart_trainer.memory_usage_history.append(allocated)
                    
                    should_stop, reason = self.smart_trainer.should_stop_training(eval_loss, train_loss)
                    
                    # Calculate validation loss trend if we have enough data
                    val_trend_info = ""
                    if len(self.smart_trainer.validation_losses) >= 3:
                        recent_vals = self.smart_trainer.validation_losses[-3:]
                        val_trend = np.polyfit(range(len(recent_vals)), recent_vals, 1)[0]
                        if val_trend < -0.001:
                            val_trend_info = f" (‚Üì {abs(val_trend):.4f}/step)"
                        elif val_trend > 0.001:
                            val_trend_info = f" (‚Üë {val_trend:.4f}/step)"
                        else:
                            val_trend_info = " (‚Üí stable)"
                    
                    print(f"\\nüìä Evaluation #{len(self.smart_trainer.validation_losses)} - Train Loss: {train_loss:.4f}, Val Loss: {eval_loss:.4f}{val_trend_info}")
                    print(f"üí≠ Decision: {reason}")
                    
                    if should_stop:
                        print(f"üõë Stopping training: {reason}")
                        self.control.should_training_stop = True
                    
                    return eval_results
                
                def log(self, logs, start_time=None):
                    """Override to track current training loss"""
                    super().log(logs, start_time)
                    # Store the latest training loss for access during evaluation
                    if "loss" in logs and not any(k.startswith("eval_") for k in logs):
                        self.smart_trainer._current_train_loss = logs["loss"]
                
                def _save_checkpoint(self, model, trial):
                    """Override to save additional training state"""
                    super()._save_checkpoint(model, trial)
                    
                    # Save training state
                    checkpoint_folder = self.state.best_model_checkpoint or self.args.output_dir
                    state_file = Path(checkpoint_folder) / "training_state.json"
                    
                    # Save complete configuration for resume
                    state_data = {
                        "step": self.state.global_step,
                        "epoch": self.state.epoch,
                        "best_loss": self.smart_trainer.best_loss,
                        "bad_epochs": self.smart_trainer.bad_epochs,
                        "training_time_minutes": (time.time() - self.smart_trainer.start_time) / 60,
                        "config": {
                            "learning_speed": self.smart_trainer.config.learning_speed,
                            "training_intensity": self.smart_trainer.config.training_intensity,
                            "mixed_precision": self.smart_trainer.config.mixed_precision,
                            "gradient_checkpointing": self.smart_trainer.config.gradient_checkpointing,
                            "gradient_accumulation_steps": self.smart_trainer.config.gradient_accumulation_steps,
                            "scheduler_type": self.smart_trainer.config.scheduler_type,
                            "use_8bit": self.smart_trainer.config.use_8bit,
                            "use_4bit": self.smart_trainer.config.use_4bit,
                            "optim": self.smart_trainer.config.optim,
                            "eval_steps": self.smart_trainer.config.eval_steps,
                            "warmup_ratio": self.smart_trainer.config.warmup_ratio,
                            "overfitting_threshold": self.smart_trainer.config.overfitting_threshold,
                            "patience_bad_results": self.smart_trainer.config.patience_bad_results,
                            "min_improvement_threshold": self.smart_trainer.config.min_improvement_threshold,
                        }
                    }
                    
                    with open(state_file, 'w') as f:
                        json.dump(state_data, f, indent=2)
            
            # Create trainer
            trainer = SmartHFTrainer(
                smart_trainer_instance=self,
                model=self.model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=val_dataset,
                data_collator=data_collator,
                processing_class=self.tokenizer,
            )
            
            print(f"\\nüöÄ Starting training with {len(train_dataset)} examples...")
            print(f"‚öôÔ∏è Learning Rate: {learning_rate}")
            print(f"üì¶ Batch Size: {batch_size}")
            print(f"‚è±Ô∏è Max Time: {self.config.max_training_time_minutes or 'unlimited'} minutes")
            if self.config.circular_training:
                print(f"üîÑ Circular Training: Up to {num_epochs} epochs")
                print(f"üìä Will cycle through dataset until overfitting occurs")
            
            # Check for resume checkpoint
            resume_from_checkpoint = None
            if hasattr(self, 'args') and hasattr(self.args, 'resume_from_checkpoint') and self.args.resume_from_checkpoint:
                if hasattr(self.args, 'checkpoint_path') and self.args.checkpoint_path:
                    # Use specific checkpoint
                    checkpoint_path = Path(self.args.checkpoint_path)
                    if checkpoint_path.exists():
                        resume_from_checkpoint = str(checkpoint_path)
                        print(f"üìÇ Resuming from checkpoint: {checkpoint_path}")
                    else:
                        print(f"‚ö†Ô∏è Checkpoint not found: {checkpoint_path}")
                else:
                    # Auto-detect latest checkpoint
                    latest_checkpoint = self.find_latest_checkpoint()
                    if latest_checkpoint:
                        resume_from_checkpoint = str(latest_checkpoint)
                        print(f"üìÇ Resuming from latest checkpoint: {latest_checkpoint}")
                    else:
                        print("‚ö†Ô∏è No checkpoints found to resume from")
            
            # Train
            if resume_from_checkpoint:
                print("üîÑ Continuing training from checkpoint...")
                trainer.train(resume_from_checkpoint=resume_from_checkpoint)
            else:
                print("üÜï Starting fresh training...")
                trainer.train()
            
            # Save final model
            print("üíæ Saving final model...")
            trainer.save_model()
            self.tokenizer.save_pretrained(str(OUTPUT_PATH))
            
            # Save training configuration for future resume
            config_file = OUTPUT_PATH / "training_config.json"
            config_data = {
                "learning_speed": self.config.learning_speed,
                "training_intensity": self.config.training_intensity,
                "mixed_precision": self.config.mixed_precision,
                "gradient_checkpointing": self.config.gradient_checkpointing,
                "gradient_accumulation_steps": self.config.gradient_accumulation_steps,
                "scheduler_type": self.config.scheduler_type,
                "use_8bit": self.config.use_8bit,
                "use_4bit": self.config.use_4bit,
                "optim": self.config.optim,
                "eval_steps": self.config.eval_steps,
                "warmup_ratio": self.config.warmup_ratio,
                "overfitting_threshold": self.config.overfitting_threshold,
                "patience_bad_results": self.config.patience_bad_results,
                "min_improvement_threshold": self.config.min_improvement_threshold,
            }
            with open(config_file, 'w') as f:
                json.dump(config_data, f, indent=2)
            print(f"üìù Training config saved to: {config_file}")
            
            # Create training report
            self.create_training_report()
            
            print("\\n" + "="*60)
            print("üéâ TRAINING COMPLETED SUCCESSFULLY!")
            print("="*60)
            print(f"üíæ Model saved to: {OUTPUT_PATH}")
            print(f"‚è±Ô∏è Training time: {(time.time() - self.start_time)/60:.1f} minutes")
            print(f"üéØ Best validation loss: {self.best_loss:.4f}")
            
        except Exception as e:
            print(f"\\n‚ùå Training failed: {e}")
            self.logger.error(f"Training failed: {e}", exc_info=True)
            raise

    def create_training_report(self):
        """Create comprehensive visual training report"""
        if len(self.training_losses) > 0 and len(self.validation_losses) > 0:
            # Create figure with subplots
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('Training Progress Report', fontsize=16)
            
            # 1. Loss curves
            ax1 = axes[0, 0]
            steps_train = list(range(len(self.training_losses)))
            steps_val = list(range(len(self.validation_losses)))
            
            ax1.plot(steps_train, self.training_losses, label='Training Loss', color='blue', alpha=0.8)
            ax1.plot(steps_val, self.validation_losses, label='Validation Loss', color='red', alpha=0.8)
            ax1.set_xlabel('Evaluation Steps')
            ax1.set_ylabel('Loss')
            ax1.set_title('Loss Curves')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # 2. Learning rate schedule
            ax2 = axes[0, 1]
            if len(self.learning_rates_history) > 0:
                steps_lr = list(range(len(self.learning_rates_history)))
                ax2.plot(steps_lr, self.learning_rates_history, color='green', alpha=0.8)
                ax2.set_xlabel('Evaluation Steps')
                ax2.set_ylabel('Learning Rate')
                ax2.set_title('Learning Rate Schedule')
                ax2.grid(True, alpha=0.3)
                ax2.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
            else:
                ax2.text(0.5, 0.5, 'No LR data', ha='center', va='center', transform=ax2.transAxes)
            
            # 3. Memory usage (if available)
            ax3 = axes[1, 0]
            if len(self.memory_usage_history) > 0:
                steps_mem = list(range(len(self.memory_usage_history)))
                ax3.plot(steps_mem, self.memory_usage_history, color='purple', alpha=0.8)
                ax3.set_xlabel('Evaluation Steps')
                ax3.set_ylabel('GPU Memory (GB)')
                ax3.set_title('GPU Memory Usage')
                ax3.grid(True, alpha=0.3)
            else:
                ax3.text(0.5, 0.5, 'No memory data', ha='center', va='center', transform=ax3.transAxes)
            
            # 4. Overfitting detection
            ax4 = axes[1, 1]
            if len(self.training_losses) > 3 and len(self.validation_losses) > 3:
                # Calculate overfitting metric
                overfitting_metric = []
                for i in range(3, len(self.validation_losses)):
                    train_avg = np.mean(self.training_losses[max(0, i-3):i])
                    val_avg = np.mean(self.validation_losses[max(0, i-3):i])
                    overfitting_metric.append(val_avg - train_avg)
                
                steps_overfit = list(range(3, len(self.validation_losses)))
                ax4.plot(steps_overfit, overfitting_metric, color='orange', alpha=0.8)
                ax4.axhline(y=self.config.overfitting_threshold, color='red', linestyle='--', label='Threshold')
                ax4.set_xlabel('Evaluation Steps')
                ax4.set_ylabel('Val Loss - Train Loss')
                ax4.set_title('Overfitting Detection')
                ax4.legend()
                ax4.grid(True, alpha=0.3)
            else:
                ax4.text(0.5, 0.5, 'Insufficient data', ha='center', va='center', transform=ax4.transAxes)
            
            plt.tight_layout()
            plot_path = OUTPUT_PATH / "training_progress.png"
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"üìà Training plot saved to: {plot_path}")
        
        # Save comprehensive training report
        report = {
            "model_name": str(MODEL_PATH),
            "training_config": {
                "learning_speed": self.config.learning_speed,
                "training_intensity": self.config.training_intensity,
                "max_examples": self.config.max_examples,
                "batch_size": self.batch_sizes[self.config.training_intensity],
                "learning_rate": self.learning_rates[self.config.learning_speed],
                "lora_rank": self.lora_ranks[self.config.training_intensity],
                "mixed_precision": self.config.mixed_precision,
                "gradient_checkpointing": self.config.gradient_checkpointing,
                "gradient_accumulation": self.config.gradient_accumulation_steps,
                "scheduler_type": self.config.scheduler_type,
                "optimizer": self.config.optim,
                "use_8bit": self.config.use_8bit,
                "use_4bit": self.config.use_4bit,
                "eval_steps": self.config.eval_steps,
                "warmup_ratio": self.config.warmup_ratio,
                "overfitting_threshold": self.config.overfitting_threshold,
                "patience_bad_results": self.config.patience_bad_results,
            },
            "device_info": {
                "device": self.device,
                "cuda_available": torch.cuda.is_available(),
                "cuda_device_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
                "cuda_capability": f"{torch.cuda.get_device_capability(0)[0]}.{torch.cuda.get_device_capability(0)[1]}" if torch.cuda.is_available() else None,
            },
            "results": {
                "training_time_minutes": (time.time() - self.start_time) / 60,
                "best_validation_loss": float(self.best_loss),
                "total_training_steps": len(self.training_losses),
                "total_validation_steps": len(self.validation_losses),
                "final_training_loss": float(self.training_losses[-1]) if self.training_losses else None,
                "final_validation_loss": float(self.validation_losses[-1]) if self.validation_losses else None,
                "improvement": float(self.validation_losses[0] - self.best_loss) if self.validation_losses else None,
            },
            "training_history": {
                "training_losses": [float(x) for x in self.training_losses],
                "validation_losses": [float(x) for x in self.validation_losses],
                "learning_rates": [float(x) for x in self.learning_rates_history] if self.learning_rates_history else [],
                "memory_usage_gb": [float(x) for x in self.memory_usage_history] if self.memory_usage_history else [],
            },
            "model_info": self.model_info if self.model_info else {},
        }
        
        report_path = OUTPUT_PATH / "training_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"üìä Training report saved to: {report_path}")

    def test_model(self, test_prompt: Optional[str] = None):
        """Quick test of trained model"""
        if self.config.skip_test:
            print("üö´ Model testing skipped")
            return None
        
        prompt = test_prompt or self.config.test_prompt
        print(f"\\nüß™ Testing trained model...")
        print(f"‚ùì Prompt: {prompt}")
        
        # Determine torch dtype
        torch_dtype = self._determine_torch_dtype()
        
        # Load trained model
        tokenizer = AutoTokenizer.from_pretrained(str(OUTPUT_PATH))
        model = AutoModelForCausalLM.from_pretrained(
            str(OUTPUT_PATH),
            torch_dtype=torch_dtype,
            device_map="auto" if self.device == "cuda" else None,
            low_cpu_mem_usage=True
        )
        
        if self.device == "mps":
            model = model.to("mps")
        
        # Generate response with saved generation config if available
        generation_kwargs = {
            "max_new_tokens": 150,
            "temperature": 0.7,
            "top_p": 0.9,
            "do_sample": True,
            "pad_token_id": tokenizer.eos_token_id
        }
        
        # Use generation config if available
        if self.model_info and 'generation_config' in self.model_info:
            gen_config = self.model_info['generation_config']
            generation_kwargs.update({
                "temperature": gen_config.get('temperature', 0.7),
                "top_p": gen_config.get('top_p', 0.9),
                "top_k": gen_config.get('top_k', None),
                "repetition_penalty": gen_config.get('repetition_penalty', 1.0),
            })
        
        # Generate response
        inputs = tokenizer.encode(f"User: {prompt}\\n\\nAssistant:", return_tensors="pt")
        if self.device in ["cuda", "mps"]:
            inputs = inputs.to(self.device)
        
        with torch.no_grad():
            if self.config.mixed_precision == "bf16" and self.device == "cuda":
                with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
                    outputs = model.generate(inputs, **generation_kwargs)
            elif self.config.mixed_precision == "fp16" and self.device == "cuda":
                with torch.autocast(device_type="cuda", dtype=torch.float16):
                    outputs = model.generate(inputs, **generation_kwargs)
            else:
                outputs = model.generate(inputs, **generation_kwargs)
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        assistant_response = response.split("Assistant:")[-1].strip()
        
        print(f"ü§ñ Response: {assistant_response}")
        return assistant_response


def main():
    """Main function"""
    print("üéØ Smart Fine-Tuning with Auto-Stop")
    print("üîó LLM System Integration")
    print("="*60)
    
    # Parse arguments
    config, args = parse_arguments()
    
    # If resuming, try to load config from checkpoint
    if args.resume_from_checkpoint:
        checkpoint_path = None
        if args.checkpoint_path:
            checkpoint_path = Path(args.checkpoint_path)
        else:
            # Find latest checkpoint
            checkpoints = list(OUTPUT_PATH.glob("checkpoint-*"))
            if checkpoints:
                checkpoint_path = max(checkpoints, key=lambda p: int(p.name.split("-")[-1]))
            elif OUTPUT_PATH.exists() and (OUTPUT_PATH / "adapter_model.safetensors").exists():
                # No checkpoints but final model exists
                checkpoint_path = OUTPUT_PATH
        
        if checkpoint_path and checkpoint_path.exists():
            saved_config = load_checkpoint_config(checkpoint_path)
            if saved_config:
                print("üìÇ Loading configuration from checkpoint...")
                print(f"   Checkpoint: {checkpoint_path}")
                
                # Override config with saved values
                config.learning_speed = saved_config.get('learning_speed', config.learning_speed)
                config.training_intensity = saved_config.get('training_intensity', config.training_intensity)
                config.mixed_precision = saved_config.get('mixed_precision', config.mixed_precision)
                config.gradient_checkpointing = saved_config.get('gradient_checkpointing', config.gradient_checkpointing)
                config.gradient_accumulation_steps = saved_config.get('gradient_accumulation_steps', config.gradient_accumulation_steps)
                config.scheduler_type = saved_config.get('scheduler_type', config.scheduler_type)
                config.use_8bit = saved_config.get('use_8bit', config.use_8bit)
                config.use_4bit = saved_config.get('use_4bit', config.use_4bit)
                config.optim = saved_config.get('optim', config.optim)
                config.eval_steps = saved_config.get('eval_steps', config.eval_steps)
                config.warmup_ratio = saved_config.get('warmup_ratio', config.warmup_ratio)
                config.overfitting_threshold = saved_config.get('overfitting_threshold', config.overfitting_threshold)
                config.patience_bad_results = saved_config.get('patience_bad_results', config.patience_bad_results)
                config.min_improvement_threshold = saved_config.get('min_improvement_threshold', config.min_improvement_threshold)
                
                print("‚úÖ Configuration loaded from checkpoint")
                config._loaded_from_checkpoint = True
            else:
                print("‚ö†Ô∏è No saved configuration found in checkpoint, using command line parameters")
                config._loaded_from_checkpoint = False
        else:
            config._loaded_from_checkpoint = False
    else:
        config._loaded_from_checkpoint = False
    
    # Show configuration
    print("‚öôÔ∏è Training Configuration:")
    print(f"   Model: {MODEL_PATH}")
    print(f"   Learning Speed: {config.learning_speed}")
    print(f"   Training Intensity: {config.training_intensity}")
    print(f"   Max Time: {config.max_training_time_minutes or 'unlimited'} minutes")
    print(f"   Data Files: {config.data_files}")
    if config.data_folders:
        print(f"   Data Folders: {', '.join(config.data_folders)}")
    print(f"   Max Examples: {config.max_examples or 'unlimited'}")
    if config.circular_training:
        print(f"   Circular Training: Yes (max {config.max_circular_epochs} epochs)")
        print(f"   Early Stop on Perfect: {'Yes' if config.early_stop_on_perfect else 'No'}")
    if args.resume_from_checkpoint:
        print(f"   Resume Training: Yes")
        if args.checkpoint_path:
            print(f"   Checkpoint Path: {args.checkpoint_path}")
        else:
            print(f"   Checkpoint Path: Auto-detect latest")
        if hasattr(config, '_loaded_from_checkpoint') and config._loaded_from_checkpoint:
            print(f"   Config Source: Loaded from checkpoint")
    
    # Show advanced settings if any are non-default
    advanced_shown = False
    if config.mixed_precision != "no":
        if not advanced_shown:
            print("\\n‚ö° Advanced Settings:")
            advanced_shown = True
        print(f"   Mixed Precision: {config.mixed_precision}")
    if config.use_4bit or config.use_8bit:
        if not advanced_shown:
            print("\\n‚ö° Advanced Settings:")
            advanced_shown = True
        print(f"   Quantization: {'4-bit' if config.use_4bit else '8-bit'}")
    if config.gradient_checkpointing:
        if not advanced_shown:
            print("\\n‚ö° Advanced Settings:")
            advanced_shown = True
        print(f"   Gradient Checkpointing: Enabled")
    if config.gradient_accumulation_steps > 1:
        if not advanced_shown:
            print("\\n‚ö° Advanced Settings:")
            advanced_shown = True
        print(f"   Gradient Accumulation: {config.gradient_accumulation_steps} steps")
    if config.scheduler_type != "cosine":
        if not advanced_shown:
            print("\\n‚ö° Advanced Settings:")
            advanced_shown = True
        print(f"   LR Scheduler: {config.scheduler_type}")
    
    print()
    
    # Check paths
    if not MODEL_PATH.exists():
        print(f"‚ùå Model not found at: {MODEL_PATH}")
        print("Please run the installer first to download a model.")
        return 1
    
    # Check for data files
    # Determine which folders to check
    check_folders = []
    if config.data_folders:
        for folder_name in config.data_folders:
            folder_path = DATA_PATH / folder_name
            if folder_path.exists():
                check_folders.append(folder_path)
    else:
        check_folders = [DATA_PATH]
        for subfolder in DATA_PATH.iterdir():
            if subfolder.is_dir() and not subfolder.name.startswith('.'):
                check_folders.append(subfolder)
    
    # Count files
    total_files = 0
    for folder in check_folders:
        total_files += len(list(folder.glob(config.data_files)))
    
    if total_files == 0:
        print(f"‚ö†Ô∏è No data files found matching '{config.data_files}'")
        print()
        print("To start fine-tuning:")
        print(f"1. Create organized subfolders in {DATA_PATH}:")
        print(f"   - alpaca/     # For instruction/output format")
        print(f"   - chatml/     # For System/User/Assistant format")
        print(f"   - qa/         # For question/answer format")
        print(f"   - stackoverflow/  # For StackOverflow Q&A")
        print(f"2. Create JSON files with training data in appropriate folders")
        print(f"3. Example formats:")
        print(f"   - {{'instruction': 'question', 'output': 'answer'}}")
        print(f"   - {{'System': 'context', 'User': 'question', 'Assistant': 'answer'}}")
        print(f"   - {{'question': 'Q title', 'answer': 'answer', 'score': 42, 'tags': ['python']}}")
        print(f"4. Run this script again")
        return 1
    else:
        print(f"‚úÖ Found {total_files} data files across {len(check_folders)} folder(s)")
    
    # Ask for confirmation unless silent
    if not args.silent:
        response = input("\\nStart training? (y/n): ").lower().strip()
        if response != 'y':
            print("Training cancelled.")
            return 0
    
    try:
        # Create trainer and start
        trainer = SmartTrainer(config)
        trainer.args = args  # Pass args for resume functionality
        trainer.train()
        
        # Test the model
        trainer.test_model()
        
        print(f"\\nüéâ Fine-tuning completed!")
        print(f"üíæ Model saved to: {OUTPUT_PATH}")
        print(f"üìä Logs saved to: {LOG_PATH}")
        print(f"\\nüîó The server will automatically detect and use the fine-tuned model")
        print(f"üöÄ Restart the server to use your fine-tuned model")
        
    except KeyboardInterrupt:
        print(f"\\nüõë Training interrupted by user")
        return 130
    except Exception as e:
        print(f"\\n‚ùå Training failed: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
'''
    
    try:
        # Save the script
        script_file = installer.scripts_path / "finetune.py"
        script_file.write_text(script_content, encoding='utf-8')
        
        # Make executable on Unix
        if platform.system() != "Windows":
            import os
            os.chmod(script_file, 0o755)
        
        # Create example data files
        create_example_data_files(installer)
        
        print_success("Fine-tuning script created")
        return True
        
    except Exception as e:
        print_error(f"Failed to create fine-tuning script: {e}")
        return False


def create_example_data_files(installer):
    """Copy training data files from installer package and create example subfolders"""
    print_progress("Setting up training data structure...")
    
    # Copy all JSON files from installer package data directory
    import shutil
    package_data_dir = Path(__file__).parent.parent / "data"
    
    if package_data_dir.exists():
        # Copy all JSON files from the data directory to root
        json_files = list(package_data_dir.glob("*.json"))
        
        if json_files:
            for src_file in json_files:
                dst_file = installer.data_path / src_file.name
                shutil.copy2(src_file, dst_file)
                print_progress(f"Copied {src_file.name}")
            
            print_success(f"Copied {len(json_files)} data files to root")
    
    # Create example subfolders with sample data
    example_folders = {
        "alpaca": [
            {
                "instruction": "What is machine learning?",
                "output": "Machine learning is a subset of artificial intelligence that enables computer systems to learn and improve from experience without being explicitly programmed. It uses algorithms to analyze data, identify patterns, and make decisions with minimal human intervention.",
                "input": ""
            },
            {
                "instruction": "Explain the difference between supervised and unsupervised learning",
                "output": "Supervised learning uses labeled data where the model learns from input-output pairs, like classification and regression. Unsupervised learning works with unlabeled data to find hidden patterns, like clustering and dimensionality reduction.",
                "input": "Context: Machine learning algorithms"
            }
        ],
        "chatml": [
            {
                "System": "You are a helpful AI assistant specializing in Python programming.",
                "User": "How do I read a CSV file in Python?",
                "Assistant": "You can read a CSV file in Python using the pandas library:\\n\\n```python\\nimport pandas as pd\\n\\n# Read CSV file\\ndf = pd.read_csv('file.csv')\\n\\n# Display first few rows\\nprint(df.head())\\n```\\n\\nAlternatively, you can use the built-in csv module for simpler operations."
            }
        ],
        "qa": [
            {
                "question": "What are the main benefits of using Docker?",
                "answer": "Docker provides several key benefits: 1) Consistency across environments, 2) Lightweight resource usage compared to VMs, 3) Fast deployment and scaling, 4) Isolation of applications, 5) Easy version control and rollback, 6) Simplified dependency management."
            }
        ],
        "stackoverflow": [
            {
                "question": "How to check if a list is empty in Python?",
                "answer": "There are several pythonic ways to check if a list is empty:\\n\\n1. **Most Pythonic - Direct boolean check:**\\n```python\\nif not my_list:\\n    print('List is empty')\\n```\\n\\n2. **Using len():**\\n```python\\nif len(my_list) == 0:\\n    print('List is empty')\\n```\\n\\n3. **Comparing to empty list:**\\n```python\\nif my_list == []:\\n    print('List is empty')\\n```\\n\\nThe first method is preferred as it's the most pythonic and works with any sequence type.",
                "score": 245,
                "tags": ["python", "list", "validation"],
                "accepted": True
            }
        ],
        "prompt_response": [
            {
                "prompt": "Write a haiku about programming",
                "response": "Code flows like water\\nBugs surface, then disappear\\nShip it anyway"
            }
        ],
        "conversations": [
            {
                "messages": [
                    {"role": "user", "content": "Can you help me understand recursion?"},
                    {"role": "assistant", "content": "I'd be happy to explain recursion! Recursion is a programming technique where a function calls itself to solve a problem by breaking it down into smaller, similar subproblems."},
                    {"role": "user", "content": "Can you give me a simple example?"},
                    {"role": "assistant", "content": "Sure! Here's a classic example - calculating factorial:\\n\\n```python\\ndef factorial(n):\\n    # Base case\\n    if n <= 1:\\n        return 1\\n    # Recursive case\\n    return n * factorial(n - 1)\\n\\n# Example: 5! = 5 √ó 4 √ó 3 √ó 2 √ó 1 = 120\\nprint(factorial(5))  # Output: 120\\n```\\n\\nThe function calls itself with a smaller number until it reaches the base case (n <= 1)."}
                ]
            }
        ],
        "plaintext": [
            {
                "text": "Machine learning is transforming how we interact with technology. At its core, machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\\n\\nThe importance of machine learning continues to grow. Because of new computing technologies, machine learning today is not like machine learning of the past. It was born from pattern recognition and the theory that computers can learn without being programmed to perform specific tasks. The iterative aspect of machine learning is important because as models are exposed to new data, they are able to independently adapt. They learn from previous computations to produce reliable, repeatable decisions and results.\\n\\nWhile machine learning is not new, the ability to automatically apply complex mathematical calculations to big data ‚Äì over and over, faster and faster ‚Äì is a recent development. This revival of interest in machine learning is due to the same factors that have made data mining and analytics more popular than ever. Things like growing volumes and varieties of available data, computational processing that is cheaper and more powerful, and affordable data storage."
            }
        ]
    }
    
    # Create folders and example files
    created_folders = []
    for folder_name, examples in example_folders.items():
        folder_path = installer.data_path / folder_name
        if not folder_path.exists():
            folder_path.mkdir(parents=True, exist_ok=True)
            created_folders.append(folder_name)
            
            # Save example file
            example_file = folder_path / f"{folder_name}_example.json"
            with open(example_file, 'w', encoding='utf-8') as f:
                json.dump(examples, f, indent=2, ensure_ascii=False)
            
            # For plaintext folder, also create a .txt example
            if folder_name == "plaintext":
                txt_example = folder_path / "example_article.txt"
                txt_content = """Introduction to Natural Language Processing

Natural Language Processing (NLP) is a fascinating field at the intersection of computer science, artificial intelligence, and linguistics. It focuses on the interaction between computers and human language, particularly how to program computers to process and analyze large amounts of natural language data.

The goal of NLP is to enable computers to understand, interpret, and generate human language in a valuable way. This technology powers many applications we use daily, from virtual assistants like Siri and Alexa to language translation services and sentiment analysis tools.

Key Components of NLP:

1. Tokenization: Breaking text into smaller units (tokens) such as words or sentences.
2. Part-of-Speech Tagging: Identifying grammatical parts of speech for each word.
3. Named Entity Recognition: Identifying and classifying named entities (people, places, organizations).
4. Sentiment Analysis: Determining the emotional tone or opinion expressed in text.
5. Machine Translation: Automatically translating text from one language to another.

Modern NLP heavily relies on machine learning, particularly deep learning techniques. Transformer-based models like BERT and GPT have revolutionized the field, achieving unprecedented performance on various NLP tasks.

The applications of NLP are vast and growing, from chatbots and content generation to information extraction and document summarization. As we continue to generate more textual data, the importance of NLP in making sense of this information becomes increasingly critical."""
                
                with open(txt_example, 'w', encoding='utf-8') as f:
                    f.write(txt_content)
                
                print_progress(f"Created {folder_name}/ with JSON and TXT examples")
            else:
                print_progress(f"Created {folder_name}/ with example data")
    
    if created_folders:
        print_success(f"Created {len(created_folders)} example folders: {', '.join(created_folders)}")
    
    # Create a README in the data folder
    readme_content = """# Training Data Organization

This folder contains training data for fine-tuning your LLM. You can organize your data by format type:

## Folder Structure

- **alpaca/** - Instruction-following format (instruction/output)
- **chatml/** - Chat markup language format (System/User/Assistant)
- **conversations/** - Multi-turn conversations (messages format)
- **qa/** - Simple question/answer pairs
- **stackoverflow/** - StackOverflow-style Q&A with metadata
- **prompt_response/** - Simple prompt/response pairs
- **plaintext/** - Plain text for continued pretraining (books, articles, docs)

## Data Formats

### Alpaca Format (alpaca/)
```json
{
  "instruction": "Your question or task",
  "output": "The expected response",
  "input": "Optional context or additional information"
}
```

### ChatML Format (chatml/)
```json
{
  "System": "System prompt or context",
  "User": "User's question",
  "Assistant": "Assistant's response"
}
```

### Conversations Format (conversations/)
```json
{
  "messages": [
    {"role": "user", "content": "First message"},
    {"role": "assistant", "content": "Response"},
    {"role": "user", "content": "Follow-up"},
    {"role": "assistant", "content": "Another response"}
  ]
}
```

### Q&A Format (qa/)
```json
{
  "question": "The question",
  "answer": "The answer"
}
```

### StackOverflow Format (stackoverflow/)
```json
{
  "question": "Question title",
  "answer": "Detailed answer with code examples",
  "score": 42,
  "tags": ["python", "machine-learning"],
  "accepted": true
}
```

### Prompt/Response Format (prompt_response/)
```json
{
  "prompt": "The prompt text",
  "response": "The response text"
}
```

### Plain Text Format (plaintext/)

For continued pretraining or domain adaptation, you can use plain text files:

1. **Text files (.txt)**: Place any .txt file in the plaintext folder. The entire content will be used as training data.
   
2. **JSON with text field**:
```json
{
  "text": "Your plain text content here. Can be articles, books, documentation, etc."
}
```

Use cases for plain text format:
- Training on books, articles, or documentation
- Domain-specific knowledge (medical texts, legal documents, technical manuals)
- Code repositories (when you want the model to learn code patterns)
- Web crawl data or Wikipedia articles
- Any unstructured text corpus for continued pretraining

## Usage

1. Add your training data to the appropriate folders based on format
2. Run the fine-tuning script: `python scripts/finetune.py`
3. To train on specific formats only: `python scripts/finetune.py --data-folders alpaca stackoverflow`
4. To train on plain text files: `python scripts/finetune.py --data-folders plaintext --data-files "*.txt"`
5. To mix formats: `python scripts/finetune.py --data-folders alpaca plaintext --data-files "*.json *.txt"`

## Tips

- Each JSON file can contain a single object or an array of objects
- Text files (.txt) are treated as single training examples (entire file content)
- Keep similar data formats in the same folder for better organization
- The script will automatically detect formats and show statistics
- Higher quality data generally leads to better fine-tuning results
- For large text files, consider splitting them into smaller chunks for better training
"""
    
    readme_path = installer.data_path / "README.md"
    readme_path.write_text(readme_content, encoding='utf-8')
    print_progress("Created data/README.md with format documentation")
    
    return