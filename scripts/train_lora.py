#!/usr/bin/env python3
"""
Universal Smart Fine-Tuning Script with LoRA
Uses handlers for model-specific training configurations
Generated by LLM Installer
"""

import os
import sys
import json
import time
import torch
import logging
import argparse
import subprocess
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import matplotlib.pyplot as plt
from dataclasses import dataclass, asdict
import numpy as np
import warnings
from tqdm import tqdm

# Suppress some warnings
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")
warnings.filterwarnings("ignore", category=UserWarning, module="torch")

# Disable flash attention for training to avoid compatibility issues
os.environ["TRANSFORMERS_USE_FLASH_ATTENTION"] = "0"

# Add installer path to sys.path
installer_path = Path(__file__).parent / "core"
if installer_path.exists():
    sys.path.insert(0, str(installer_path))

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('training.log')
    ]
)
logger = logging.getLogger(__name__)


class TrainingMetrics:
    """Track training metrics for auto-stop decisions."""
    
    def __init__(self):
        self.train_losses = []
        self.val_losses = []
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.evaluations = 0
        self.circular_epochs = 0
        self.start_time = time.time()
        
    def update(self, train_loss: float, val_loss: float):
        """Update metrics with new values."""
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)
        self.evaluations += 1
        
        # Check if improved
        if val_loss < self.best_val_loss:
            self.best_val_loss = val_loss
            self.patience_counter = 0
            return True
        else:
            self.patience_counter += 1
            return False
    
    def calculate_trend(self, values: List[float], window: int = 5) -> float:
        """Calculate trend through linear regression."""
        if len(values) < window:
            return 0.0
        
        recent = values[-window:]
        x = np.arange(len(recent))
        slope, _ = np.polyfit(x, recent, 1)
        
        return slope
    
    def check_overfitting(self, threshold: float = 0.1) -> Tuple[bool, str]:
        """Check for overfitting."""
        if self.evaluations < 5:
            return False, "insufficient_data"
        
        # Analyze trend
        val_trend = self.calculate_trend(self.val_losses)
        
        # Analyze gap
        recent_train = np.mean(self.train_losses[-5:])
        recent_val = np.mean(self.val_losses[-5:])
        
        # Handle edge cases
        if recent_train < 1e-6:  # Practically zero
            if recent_val < 1e-6:
                # Both are near zero - perfect fit, no overfitting
                return False, "perfect_fit"
            else:
                # Train is zero but val is not - likely overfitting
                return True, "severe_overfitting"
        
        # Calculate relative gap
        gap = (recent_val - recent_train) / recent_train
        
        # Decision based on trend and gap
        if val_trend > 0.001 and gap > threshold:
            if gap > 0.5:
                return True, "severe_overfitting"
            else:
                return True, "moderate_overfitting"
        
        # Additional check for text generation - look at perplexity
        if recent_val > 1.0:  # Likely a text generation task (higher loss)
            # Check if validation perplexity is increasing significantly
            val_perplexity = np.exp(recent_val)
            if val_perplexity > 100 and val_trend > 0.01:
                return True, "perplexity_explosion"
        
        return False, "no_overfitting"
    
    def get_status_string(self) -> str:
        """Get formatted status string."""
        elapsed = time.time() - self.start_time
        elapsed_str = f"{int(elapsed//3600):02d}:{int((elapsed%3600)//60):02d}:{int(elapsed%60):02d}"
        
        status_parts = [
            f"📊 Evaluation #{self.evaluations}",
            f"⏱️  Time: {elapsed_str}",
            f"📉 Train Loss: {self.train_losses[-1]:.4f}" if self.train_losses else "",
            f"📈 Val Loss: {self.val_losses[-1]:.4f}" if self.val_losses else "",
            f"🎯 Best Val: {self.best_val_loss:.4f}",
            f"⏳ Patience: {self.patience_counter}"
        ]
        
        # Add perplexity for text generation tasks (when loss > 1)
        if self.val_losses and self.val_losses[-1] > 1.0:
            perplexity = np.exp(self.val_losses[-1])
            status_parts.insert(4, f"📖 Perplexity: {perplexity:.1f}")
        
        return " | ".join(filter(None, status_parts))


def load_model_and_tokenizer(model_path: str, training_config: Dict[str, Any], model_info: Dict[str, Any]):
    """Load model, tokenizer, and prepare for training using handlers."""
    # Import here to avoid circular imports
    from model_loader import load_model, get_handler
    
    # Get handler
    handler = get_handler(model_info)
    if not handler:
        # Fallback to base handler
        from handlers.base import BaseHandler
        handler = BaseHandler(model_info)
        logger.warning("No specific handler found, using base handler")
    
    # Get training parameters from handler
    training_params = handler.get_training_parameters()
    logger.info(f"Using handler: {handler.__class__.__name__}")
    logger.info(f"Handler training params: {training_params}")
    
    # Load model and tokenizer (without LoRA for training)
    # Disable flash attention for training
    os.environ["USE_FLASH_ATTENTION"] = "0"
    os.environ["FLASH_ATTENTION_SKIP_RESHAPE"] = "1"
    
    try:
        model, tokenizer = load_model(
            model_info,
            model_path=model_path,
            device=training_config.get('device', 'auto'),
            dtype=training_config.get('dtype', 'auto'),
            load_in_8bit=training_config.get('use_8bit', False),
            load_in_4bit=training_config.get('use_4bit', False),
            load_lora=False,  # Don't load existing LoRA for training
            use_flash_attention_2=False  # Explicitly disable flash attention
        )
    except (ImportError, Exception) as e:
        if "flash_attn" in str(e) or "_ZN3c105ErrorC2ENS_14SourceLocationE" in str(e):
            logger.warning("Flash attention import error, patching transformers to disable it")
            
            # Monkey patch transformers to prevent flash_attn import
            import sys
            import types
            
            # Create dummy flash_attn module
            dummy_flash_attn = types.ModuleType('flash_attn')
            sys.modules['flash_attn'] = dummy_flash_attn
            sys.modules['flash_attn.bert_padding'] = types.ModuleType('flash_attn.bert_padding')
            sys.modules['flash_attn.flash_attn_interface'] = types.ModuleType('flash_attn.flash_attn_interface')
            
            # Add dummy functions to prevent import errors
            sys.modules['flash_attn.bert_padding'].index_first_axis = lambda x, y: x
            sys.modules['flash_attn.bert_padding'].pad_input = lambda x, y, z: (x, y, z)
            sys.modules['flash_attn.bert_padding'].unpad_input = lambda x, y: x
            
            # Force transformers to think flash_attn is not available
            import transformers.utils.import_utils as import_utils
            import_utils._flash_attn_2_available = False
            
            # Clear any cached imports
            for module in list(sys.modules.keys()):
                if 'qwen3' in module or 'modeling_flash_attention' in module:
                    del sys.modules[module]
            
            model, tokenizer = load_model(
                model_info,
                model_path=model_path,
                device=training_config.get('device', 'auto'),
                dtype=training_config.get('dtype', 'auto'),
                load_in_8bit=training_config.get('use_8bit', False),
                load_in_4bit=training_config.get('use_4bit', False),
                load_lora=False,  # Don't load existing LoRA for training
                use_flash_attention_2=False
            )
        else:
            raise
    
    # Prepare model for training using handler
    model = handler.prepare_model_for_training(model, training_config)
    
    # Setup tokenizer using handler config
    tokenizer_config = handler.get_tokenizer_config()
    for key, value in tokenizer_config.items():
        if hasattr(tokenizer, key):
            setattr(tokenizer, key, value)
    
    # Add padding token if needed
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Set padding_side to 'left' for autoregressive models
    tokenizer.padding_side = 'left'
    logger.info(f"Set tokenizer padding_side to 'left' for autoregressive generation")
    
    return model, tokenizer, handler, training_params


def setup_lora(model, training_config: Dict[str, Any], training_params: Dict[str, Any]):
    """Setup LoRA/QLoRA for the model."""
    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
    from core.lora_utils import find_all_linear_names
    
    # Disable cache for training to avoid warnings with gradient checkpointing
    if hasattr(model.config, 'use_cache'):
        model.config.use_cache = False
    
    # Prepare model for k-bit training if using quantization
    if training_config.get('use_8bit') or training_config.get('use_4bit'):
        model = prepare_model_for_kbit_training(model)
    
    # Get target modules from handler or use config
    target_modules = training_params.get('lora_target_modules')
    if not target_modules:
        target_modules = training_config.get('lora_target_modules')
        if not target_modules:
            # Use auto-discovery as final fallback
            logger.info("No target modules specified, auto-discovering linear layers...")
            quantization_config = None
            if training_config.get('use_8bit') or training_config.get('use_4bit'):
                # Model is quantized, pass quantization config if available
                quantization_config = getattr(model.config, 'quantization_config', None)
            target_modules = find_all_linear_names(model, quantization_config)
            logger.info(f"Auto-discovered target modules: {target_modules}")
    
    logger.info(f"LoRA target modules: {target_modules}")
    
    # Create LoRA config
    lora_config = LoraConfig(
        r=training_config['lora_r'],
        lora_alpha=training_config['lora_alpha'],
        target_modules=target_modules,
        lora_dropout=training_config['lora_dropout'],
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        modules_to_save=training_params.get('lora_modules_to_save'),
    )
    
    # Apply LoRA
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    return model


def load_dataset(data_path: str, training_config: Dict[str, Any], tokenizer, handler_params: Dict[str, Any]):
    """Load and prepare dataset."""
    from dataset_manager import DatasetManager
    from datasets import Dataset
    import glob
    
    # Create dataset manager
    manager = DatasetManager(
        model_type=training_config.get('model_type', ''),
        model_family=training_config.get('model_family', '')
    )
    
    # Check supported formats from handler
    supported_formats = handler_params.get('dataset_formats', ['alpaca', 'chat', 'completion', 'text'])
    logger.info(f"Handler supported formats: {supported_formats}")
    
    # Parse data path - handle comma-separated list or glob patterns
    data_paths = []
    if ',' in data_path:
        # Comma-separated list
        data_paths = [p.strip() for p in data_path.split(',')]
        logger.info(f"Loading from {len(data_paths)} specified files")
    elif '*' in data_path:
        # Glob pattern
        data_paths = glob.glob(data_path)
        logger.info(f"Found {len(data_paths)} files matching pattern: {data_path}")
    else:
        # Single path (file or directory)
        data_paths = data_path
    
    # Load data
    train_data, val_data = manager.load_dataset(
        data_paths,
        format=training_config.get('dataset_format', 'auto'),
        validation_split=training_config.get('validation_split', 0.1),
        max_examples=training_config.get('max_examples'),
        shuffle=True
    )
    
    # Prepare for model
    train_data = manager.prepare_for_model(train_data, tokenizer)
    val_data = manager.prepare_for_model(val_data, tokenizer)
    
    # Convert to HF datasets
    train_dataset = Dataset.from_list(train_data)
    val_dataset = Dataset.from_list(val_data) if val_data else None
    
    # Tokenize
    def tokenize_function(examples):
        # Add EOS token to each text
        texts_with_eos = [text + tokenizer.eos_token for text in examples['text']]
        return tokenizer(
            texts_with_eos,
            truncation=True,
            padding='max_length',
            max_length=training_config.get('max_seq_length', 2048)
        )
    
    train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])
    if val_dataset:
        val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['text'])
    
    return train_dataset, val_dataset


class SmartTrainer:
    """Custom trainer with auto-stop and circular training."""
    
    def __init__(self, model, tokenizer, train_dataset, val_dataset, 
                 training_config, output_dir, handler_params):
        self.model = model
        self.tokenizer = tokenizer
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.training_config = training_config
        self.output_dir = Path(output_dir)
        self.metrics = TrainingMetrics()
        self.handler_params = handler_params
        
        # Adjust overfitting threshold based on task type
        self._adjust_thresholds_for_task()
        
        # Setup training
        self._setup_training()
    
    def _adjust_thresholds_for_task(self):
        """Adjust training thresholds based on task type."""
        # Skip adjustments if force_epochs is enabled
        if self.training_config.get('force_epochs', False):
            logger.info("Force epochs enabled - auto-stop features disabled")
            return
            
        dataset_format = self.training_config.get('dataset_format', 'auto')
        model_type = self.training_config.get('model_type', '')
        
        # Text generation tasks need more lenient thresholds
        if dataset_format in ['text', 'completion'] or 'language-model' in model_type:
            # Only adjust if using default values
            if self.training_config.get('overfitting_threshold', 0.1) == 0.1:
                self.training_config['overfitting_threshold'] = 0.2  # 20% gap is OK for text gen
                logger.info("Adjusted overfitting threshold to 0.2 for text generation task")
            
            # Adjust patience as text generation converges slower
            if self.training_config.get('patience', 3) == 3:
                self.training_config['patience'] = 5
                logger.info("Adjusted patience to 5 for text generation task")
    
    def _setup_training(self):
        """Setup training components."""
        from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling
        
        # Log checkpoint settings
        save_limit = self.training_config.get('save_total_limit')
        print(f"💾 Checkpoint settings: save every {self.training_config.get('save_steps', 10)} steps")
        if save_limit is None:
            print("💾 Keeping ALL checkpoints (no limit)")
        else:
            print(f"💾 Keeping last {save_limit} checkpoints")
        
        # Check if we have validation dataset for evaluation
        has_eval_dataset = self.val_dataset is not None
        
        # Adjust evaluation strategy based on dataset availability
        eval_strategy = self.training_config['eval_strategy'] if has_eval_dataset else "no"
        if not has_eval_dataset and self.training_config['eval_strategy'] != "no":
            logger.warning("No validation dataset provided, disabling evaluation")
        
        # Check if flash attention is supported
        use_flash_attention = (
            self.handler_params.get('supports_flash_attention', True) and 
            not (self.training_config.get('use_8bit') or self.training_config.get('use_4bit'))
        )
        
        # Create training arguments
        training_args = TrainingArguments(
            output_dir=str(self.output_dir / "checkpoints"),
            num_train_epochs=self.training_config['num_train_epochs'],
            per_device_train_batch_size=self.training_config['batch_size'],
            per_device_eval_batch_size=self.training_config['batch_size'],
            gradient_accumulation_steps=self.training_config['gradient_accumulation_steps'],
            warmup_ratio=self.training_config['warmup_ratio'],
            weight_decay=self.training_config['weight_decay'],
            logging_dir=str(self.output_dir / "logs"),
            logging_steps=self.training_config['logging_steps'],
            eval_strategy=eval_strategy,  # Use adjusted strategy
            eval_steps=self.training_config['eval_steps'] if has_eval_dataset else None,
            save_strategy=self.training_config['save_strategy'],
            save_steps=self.training_config['save_steps'],
            save_total_limit=self.training_config['save_total_limit'],
            load_best_model_at_end=self.training_config['load_best_model_at_end'],
            metric_for_best_model="loss",
            greater_is_better=False,
            report_to=self.training_config['report_to'],
            remove_unused_columns=False,
            label_names=["labels"],
            learning_rate=self.training_config['learning_rate'],
            lr_scheduler_type=self.training_config['lr_scheduler_type'],
            optim=self.training_config['optimizer'],
            gradient_checkpointing=self.training_config['gradient_checkpointing'],
            # Precision - use handler recommended if available
            fp16=(self.training_config['mixed_precision'] == 'fp16' and 
                  self.handler_params.get('training_precision', 'auto') != 'bf16'),
            bf16=(self.training_config['mixed_precision'] == 'bf16' or 
                  self.handler_params.get('training_precision', 'auto') == 'bf16'),
            # Resume
            resume_from_checkpoint=self.training_config.get('resume_from_checkpoint'),
        )
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
            pad_to_multiple_of=8
        )
        
        # Create trainer with custom callback
        auto_stop_callback = AutoStopCallback(self)
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.val_dataset,
            data_collator=data_collator,
            processing_class=self.tokenizer,  # Changed from tokenizer to processing_class
            callbacks=[auto_stop_callback._callback],
        )
    
    def train(self):
        """Run training with smart features."""
        if self.training_config['circular_training']:
            self._circular_training()
        else:
            self._standard_training()
    
    def _standard_training(self):
        """Standard training."""
        print("🚀 Starting training...")
        try:
            self.trainer.train(resume_from_checkpoint=self.training_config.get('resume_from_checkpoint'))
        except torch.cuda.OutOfMemoryError as e:
            print(f"\n⚠️  CUDA Out of Memory Error during training!")
            print(f"   Try running with smaller batch size: --batch-size 1")
            print(f"   Or use quantization: --use-8bit or --method qlora")
            raise e
        
        # Save final model
        self.save_final_model()
        
        # Plot training history
        self.plot_training_history()
    
    def _circular_training(self):
        """Circular training for small datasets."""
        print("🔄 Starting circular training...")
        print("   This mode is for small datasets. The dataset will be repeated.")
        print("   Training will run for a large number of steps and stop automatically when the model converges.")

        # Set a large number of epochs and let the auto-stop callback handle termination
        self.trainer.args.num_train_epochs = self.training_config.get('max_circular_epochs', 100)
        self.trainer.args.logging_first_step = True
        self.trainer.args.logging_strategy = "steps"
        self.trainer.args.evaluation_strategy = "steps"
        self.trainer.args.save_strategy = "steps"

        # Use a smaller evaluation frequency for circular training
        self.trainer.args.eval_steps = self.training_config.get('eval_steps', 20)
        self.trainer.args.save_steps = self.trainer.args.eval_steps

        print(f"   Training for up to {self.trainer.args.num_train_epochs} cycles (epochs).")
        print(f"   Evaluating every {self.trainer.args.eval_steps} steps.")

        self._standard_training()
    
    def save_best_model(self):
        """Save best model."""
        best_path = self.output_dir / "best"
        best_path.mkdir(parents=True, exist_ok=True)
        
        self.model.save_pretrained(best_path)
        self.tokenizer.save_pretrained(best_path)
        
        # Save training state
        state = {
            'best_val_loss': self.metrics.best_val_loss,
            'evaluations': self.metrics.evaluations,
            'train_losses': self.metrics.train_losses,
            'val_losses': self.metrics.val_losses,
        }
        with open(best_path / "training_state.json", 'w') as f:
            json.dump(state, f, indent=2)
    
    def save_final_model(self):
        """Save final model to lora directory."""
        print("\n💾 Saving final model...")
        
        # Save to lora directory
        lora_path = self.output_dir
        lora_path.mkdir(parents=True, exist_ok=True)
        
        self.model.save_pretrained(lora_path)
        
        # Save adapter config with detailed info
        adapter_config = {
            'base_model': self.training_config.get('model_id', 'unknown'),
            'created_at': datetime.now().isoformat(),
            'training_config': {k: v for k, v in self.training_config.items() 
                              if k not in ['model', 'tokenizer']},  # Exclude non-serializable
            'handler_params': self.handler_params,
            'final_metrics': {
                'best_val_loss': self.metrics.best_val_loss,
                'total_evaluations': self.metrics.evaluations,
                'training_time': time.time() - self.metrics.start_time,
                'final_train_loss': self.metrics.train_losses[-1] if self.metrics.train_losses else None,
                'final_val_loss': self.metrics.val_losses[-1] if self.metrics.val_losses else None,
            }
        }
        
        with open(lora_path / "adapter_info.json", 'w') as f:
            json.dump(adapter_config, f, indent=2)
        
        print(f"✅ Model saved to {lora_path}")
        
        # Check which format was saved
        if (lora_path / "adapter_model.safetensors").exists():
            print("📄 Format: adapter_model.safetensors (recommended)")
        elif (lora_path / "adapter_model.bin").exists():
            print("📄 Format: adapter_model.bin (legacy)")
        else:
            print("⚠️  Warning: No adapter file found!")
    
    def plot_training_history(self):
        """Plot and save training history."""
        if len(self.metrics.train_losses) < 2:
            return
        
        plt.figure(figsize=(10, 6))
        plt.plot(self.metrics.train_losses, label='Train Loss', alpha=0.8)
        if self.metrics.val_losses:
            plt.plot(self.metrics.val_losses, label='Validation Loss', alpha=0.8)
        
        plt.xlabel('Evaluation Step')
        plt.ylabel('Loss')
        plt.title('Training History')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plot_path = self.output_dir / "training_history.png"
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"📊 Training plot saved to {plot_path}")


class AutoStopCallback:
    """Callback for auto-stop logic."""
    
    def __init__(self, smart_trainer):
        from transformers import TrainerCallback
        
        self.smart_trainer = smart_trainer
        self.last_eval_step = 0
        
        # Create the actual callback class
        class _AutoStopCallback(TrainerCallback):
            def __init__(self, parent):
                self.parent = parent
            
            def on_evaluate(self, args, state, control, metrics=None, **kwargs):
                """Called after evaluation."""
                if metrics and state.global_step > self.parent.last_eval_step:
                    self.parent.last_eval_step = state.global_step
                    
                    # Update metrics - get average train loss from recent history
                    train_losses = [log.get('loss', 0) for log in state.log_history 
                                   if 'loss' in log and log.get('loss') is not None]
                    train_loss = np.mean(train_losses[-10:]) if train_losses else 0
                    val_loss = metrics.get('eval_loss', 0)
                    
                    improved = self.parent.smart_trainer.metrics.update(train_loss, val_loss)
                    
                    # Print status
                    print(f"\n{self.parent.smart_trainer.metrics.get_status_string()}")
                    
                    # Skip auto-stop checks if force_epochs is enabled
                    if self.parent.smart_trainer.training_config.get('force_epochs', False):
                        print("🔒 Force epochs mode - ignoring auto-stop conditions")
                    else:
                        # Check overfitting
                        is_overfitting, reason = self.parent.smart_trainer.metrics.check_overfitting(
                            self.parent.smart_trainer.training_config['overfitting_threshold']
                        )
                        
                        if is_overfitting:
                            print(f"⚠️  Detected {reason}! Stopping training...")
                            control.should_training_stop = True
                        
                        # Check patience
                        if self.parent.smart_trainer.metrics.patience_counter >= self.parent.smart_trainer.training_config['patience']:
                            print(f"⏹️  No improvement for {self.parent.smart_trainer.training_config['patience']} evaluations. Stopping...")
                            control.should_training_stop = True
                    
                    # Save best model
                    if improved:
                        print("💾 New best model! Saving...")
                        self.parent.smart_trainer.save_best_model()
                
                return control
        
        # Create instance
        self._callback = _AutoStopCallback(self)


def extract_test_prompts_from_dataset(data_path: str, training_config: Dict, num_prompts: int = 5) -> List[Tuple[str, str]]:
    """Extract test prompts and expected completions from dataset."""
    try:
        from core.dataset_manager import DatasetManager
        manager = DatasetManager(
            model_type=training_config.get('model_type', ''),
            model_family=training_config.get('model_family', '')
        )
        
        # Load examples from dataset
        examples, _ = manager.load_dataset(
            data_path,
            format=training_config.get('dataset_format', 'auto'),
            validation_split=0,
            max_examples=num_prompts * 2,  # Load more to have variety
            shuffle=True
        )
        
        test_prompts = []
        
        for example in examples:
            prompt = None
            expected = None
            
            # Extract prompt and expected completion based on format
            if 'prompt' in example and 'completion' in example:
                prompt = example['prompt']
                expected = example['completion']
            elif 'instruction' in example:
                prompt = example['instruction']
                if 'input' in example and example['input']:
                    prompt += "\n\nInput: " + example['input']
                expected = example.get('output', example.get('response', ''))
            elif 'messages' in example and isinstance(example['messages'], list):
                # Extract last user message and assistant response
                user_msgs = [m for m in example['messages'] if m.get('role') == 'user']
                asst_msgs = [m for m in example['messages'] if m.get('role') == 'assistant']
                if user_msgs and asst_msgs:
                    prompt = user_msgs[-1].get('content', '')
                    expected = asst_msgs[-1].get('content', '')
            elif 'question' in example and 'answer' in example:
                prompt = example['question']
                expected = example['answer']
            elif 'input' in example and 'output' in example:
                prompt = example['input']
                expected = example['output']
            
            if prompt and expected:
                test_prompts.append((prompt, expected))
                if len(test_prompts) >= num_prompts:
                    break
        
        return test_prompts
    except Exception as e:
        logger.warning(f"Could not extract test prompts: {e}")
        return []


def test_model(model_path: str, lora_path: str, test_prompt: str, handler_params: Dict[str, Any], 
               training_config: Optional[Dict] = None, data_path: Optional[str] = None):
    """Test the trained model and compare with base model."""
    print("\n" + "="*60)
    print("🧪 TESTING FINE-TUNED MODEL")
    print("="*60)
    
    # Check if LoRA files exist
    lora_path_obj = Path(lora_path)
    adapter_exists = (lora_path_obj / "adapter_model.safetensors").exists() or (lora_path_obj / "adapter_model.bin").exists()
    if not adapter_exists:
        print("\n❌ ERROR: No adapter files found!")
        print(f"   Checked: {lora_path}")
        print("   Training may have failed or files were not saved correctly.")
        return
    
    from model_loader import load_model
    
    # Load model info - check both locations
    model_info_path = Path("model_info.json")
    if not model_info_path.exists():
        model_info_path = Path(model_path) / "model_info.json"
    
    with open(model_info_path, 'r') as f:
        model_info = json.load(f)
    
    # Extract test prompts from dataset
    dataset_prompts = []
    if data_path and training_config:
        print("\n🔍 Extracting test prompts from your dataset...")
        dataset_prompts = extract_test_prompts_from_dataset(data_path, training_config, num_prompts=5)
        if dataset_prompts:
            print(f"✅ Found {len(dataset_prompts)} test prompts from dataset")
        else:
            print("⚠️  Could not extract prompts from dataset, using default prompt")
    
    # Prepare all test prompts
    all_test_prompts = []
    if dataset_prompts:
        # Use dataset prompts primarily
        all_test_prompts.extend([(p, e) for p, e in dataset_prompts[:3]])  # First 3 from dataset
    if test_prompt and test_prompt.strip():
        # Add user-provided prompt
        all_test_prompts.append((test_prompt, None))
    
    if not all_test_prompts:
        all_test_prompts = [("Hello, how are you?", None)]  # Fallback
    
    # Test 1: Compare base model vs fine-tuned on multiple prompts
    print("\n1️⃣ Comparing BASE vs FINE-TUNED model responses:")
    print("-" * 50)
    
    # Load base model (without LoRA)
    print("Loading base model...")
    base_model, base_tokenizer = load_model(
        model_info,
        model_path=model_path,
        load_lora=False  # Don't load LoRA
    )
    
    max_new_tokens = handler_params.get('max_generation_length', 128)
    comparison_results = []
    
    # Test each prompt with base model
    print("\nTesting with BASE model:")
    for i, (prompt, expected) in enumerate(all_test_prompts):
        inputs = base_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = base_model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                pad_token_id=base_tokenizer.pad_token_id,
                eos_token_id=base_tokenizer.eos_token_id,
            )
        
        response = base_tokenizer.decode(outputs[0], skip_special_tokens=True)
        comparison_results.append({
            'prompt': prompt,
            'expected': expected,
            'base_response': response,
            'is_from_dataset': i < len(dataset_prompts)
        })
    
    # Clear GPU memory
    del base_model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # Load fine-tuned model (with LoRA)
    print("\nLoading fine-tuned model...")
    ft_model, ft_tokenizer = load_model(
        model_info,
        model_path=model_path,
        lora_path=lora_path,
        load_lora=True
    )
    
    # Test each prompt with fine-tuned model
    print("Testing with FINE-TUNED model:")
    for i, result in enumerate(comparison_results):
        inputs = ft_tokenizer(result['prompt'], return_tensors="pt", truncation=True, max_length=512)
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = ft_model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                pad_token_id=ft_tokenizer.pad_token_id,
                eos_token_id=ft_tokenizer.eos_token_id,
            )
        
        response = ft_tokenizer.decode(outputs[0], skip_special_tokens=True)
        result['ft_response'] = response
    
    # Display comparison results
    print("\n" + "="*60)
    print("📊 COMPARISON RESULTS")
    print("="*60)
    
    identical_count = 0
    dataset_match_count = 0
    
    for i, result in enumerate(comparison_results):
        print(f"\n{'='*50}")
        if result['is_from_dataset']:
            print(f"Test {i+1} (FROM YOUR DATASET):")
        else:
            print(f"Test {i+1} (User provided):")
        print(f"{'='*50}")
        
        # Truncate long prompts for display
        prompt_display = result['prompt'][:100] + "..." if len(result['prompt']) > 100 else result['prompt']
        print(f"\n📝 Prompt: {prompt_display}")
        
        if result['expected']:
            expected_display = result['expected'][:150] + "..." if len(result['expected']) > 150 else result['expected']
            print(f"\n✅ Expected (from dataset): {expected_display}")
        
        # Extract just the generated part (remove prompt from response)
        base_generated = result['base_response'][len(result['prompt']):].strip()
        ft_generated = result['ft_response'][len(result['prompt']):].strip()
        
        # Truncate for display
        base_display = base_generated[:150] + "..." if len(base_generated) > 150 else base_generated
        ft_display = ft_generated[:150] + "..." if len(ft_generated) > 150 else ft_generated
        
        print(f"\n🤖 BASE model: {base_display}")
        print(f"\n🎯 FINE-TUNED: {ft_display}")
        
        # Check if responses are identical
        if base_generated.strip() == ft_generated.strip():
            print("\n⚠️  IDENTICAL responses!")
            identical_count += 1
        else:
            print("\n✅ DIFFERENT responses!")
        
        # Check if fine-tuned matches expected (for dataset prompts)
        if result['expected'] and result['is_from_dataset']:
            # Simple similarity check (you could use more sophisticated metrics)
            if result['expected'].strip().lower() in ft_generated.lower():
                print("✅ Fine-tuned response matches expected output!")
                dataset_match_count += 1
            elif ft_generated.lower() in result['expected'].strip().lower():
                print("✅ Fine-tuned response is close to expected output!")
                dataset_match_count += 1
            else:
                print("⚠️  Fine-tuned response differs from expected output")
    
    # Test 2: Summary of results
    print("\n\n2️⃣ SUMMARY OF COMPARISONS:")
    print("-" * 50)
    
    total_tests = len(comparison_results)
    dataset_tests = sum(1 for r in comparison_results if r['is_from_dataset'])
    
    print(f"Total tests performed: {total_tests}")
    print(f"Tests from your dataset: {dataset_tests}")
    print(f"Identical responses: {identical_count}/{total_tests} ({identical_count/total_tests*100:.1f}%)")
    
    if dataset_tests > 0:
        print(f"Dataset matches: {dataset_match_count}/{dataset_tests} ({dataset_match_count/dataset_tests*100:.1f}%)")
    
    if identical_count == total_tests:
        print("\n❌ WARNING: All responses are identical!")
        print("   The model likely did NOT learn from your dataset.")
    elif identical_count > total_tests / 2:
        print("\n⚠️  Most responses are identical.")
        print("   The model may have learned very little.")
    else:
        print("\n✅ Model shows different behavior after training!")
        if dataset_match_count > 0:
            print(f"   And it matches {dataset_match_count} expected outputs from your dataset!")
    
    # Test 3: Check if model learned specific patterns
    print("\n3️⃣ Checking if model learned from dataset:")
    print("-" * 50)
    
    # Read adapter info to see training metrics
    adapter_info_path = Path(lora_path) / "adapter_info.json"
    if adapter_info_path.exists():
        with open(adapter_info_path, 'r') as f:
            adapter_info = json.load(f)
        
        final_metrics = adapter_info.get('final_metrics', {})
        print(f"Final training loss: {final_metrics.get('final_train_loss', 'N/A')}")
        print(f"Final validation loss: {final_metrics.get('final_val_loss', 'N/A')}")
        print(f"Best validation loss: {final_metrics.get('best_val_loss', 'N/A')}")
        print(f"Training time: {final_metrics.get('training_time', 0)/60:.1f} minutes")
        
        # Check if loss decreased
        if isinstance(final_metrics.get('final_train_loss'), (int, float)):
            if final_metrics['final_train_loss'] < 0.01:
                print("\n⚠️  WARNING: Training loss is extremely low (< 0.01)")
                print("   This might indicate overfitting on a small dataset.")
            elif final_metrics['final_train_loss'] > 2.0:
                print("\n⚠️  WARNING: Training loss is still high (> 2.0)")
                print("   The model may need more training.")
    
    # Test 4: Perplexity comparison
    print("\n4️⃣ Calculating perplexity on your dataset:")
    print("-" * 50)
    
    if data_path and training_config:
        try:
            # Calculate perplexity on a sample of training data
            from core.dataset_manager import DatasetManager
            manager = DatasetManager(
                model_type=training_config.get('model_type', ''),
                model_family=training_config.get('model_family', '')
            )
            
            # Load a few examples
            test_data, _ = manager.load_dataset(
                data_path,
                format=training_config.get('dataset_format', 'auto'),
                validation_split=0,
                max_examples=10,
                shuffle=True
            )
            
            if test_data and len(test_data) > 0:
                total_loss = 0
                num_tokens = 0
                
                for example in test_data[:5]:  # Test on 5 examples
                    # Extract text from example
                    if 'text' in example:
                        text = example['text']
                    elif 'prompt' in example and 'completion' in example:
                        text = example['prompt'] + ' ' + example['completion']
                    elif 'messages' in example:
                        text = ' '.join([m.get('content', '') for m in example['messages']])
                    else:
                        continue
                    
                    # Tokenize and get loss
                    inputs = ft_tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
                    if torch.cuda.is_available():
                        inputs = {k: v.cuda() for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = ft_model(**inputs, labels=inputs['input_ids'])
                        total_loss += outputs.loss.item() * inputs['input_ids'].size(1)
                        num_tokens += inputs['input_ids'].size(1)
                
                if num_tokens > 0:
                    avg_loss = total_loss / num_tokens
                    perplexity = np.exp(avg_loss)
                    print(f"Average loss on dataset samples: {avg_loss:.4f}")
                    print(f"Perplexity on dataset samples: {perplexity:.2f}")
                    
                    if perplexity < 1.5:
                        print("✅ Excellent! Model has learned the dataset very well.")
                    elif perplexity < 3.0:
                        print("✅ Good! Model has learned from the dataset.")
                    elif perplexity < 10.0:
                        print("⚠️  Moderate learning. Consider more training.")
                    else:
                        print("❌ High perplexity. Model may not have learned much.")
        except Exception as e:
            print(f"Could not calculate perplexity: {e}")
    
    print("\n✅ Testing complete!")
    
    # Diagnostic summary
    print("\n" + "="*60)
    print("📋 DIAGNOSTIC SUMMARY")
    print("="*60)
    
    # Check if responses are different
    if base_response.strip() == ft_response.strip():
        print("❌ Fine-tuned model gives IDENTICAL response to base model")
        print("   → Model likely did NOT learn from your dataset")
    else:
        print("✅ Fine-tuned model gives DIFFERENT response from base model")
        print("   → Model has been modified by training")
    
    print("\n💡 Troubleshooting tips if model didn't learn:")
    print("1. Check dataset format:")
    print(f"   - Your format: {training_config.get('dataset_format', 'auto')}")
    print("   - Run with --debug to see how data is being loaded")
    print("\n2. Increase training intensity:")
    print("   - Use more epochs: --epochs 5 or --mode slow")
    print("   - Increase learning rate: --learning-rate 5e-4")
    print("   - Increase LoRA rank: --lora-r 64")
    print("\n3. Check training logs:")
    print("   - Look for decreasing loss in training_history.png")
    print("   - Check tensorboard: tensorboard --logdir ./lora/logs")
    print("\n4. Verify dataset:")
    print("   - Ensure your data has clear input-output patterns")
    print("   - Check that prompts and completions are properly formatted")


def main():
    """Main training function."""
    parser = argparse.ArgumentParser(description="Universal Fine-tune model with LoRA")
    
    # Add config file argument first
    parser.add_argument("--config", type=str, default=None,
                       help="Path to a YAML configuration file.")
    
    # All other arguments are optional if a config file is provided
    parser.add_argument("--data", type=str, help="Path to training data (file or directory)")
    parser.add_argument("--output", type=str, help="Output directory for LoRA adapter")
    parser.add_argument("--model-path", type=str, default="./model", help="Path to base model")
    
    # Training mode
    parser.add_argument("--mode", type=str,
                       choices=["slow", "medium", "fast", "circle", "non-stop", "adaptive",
                                "quick", "balanced", "quality"],
                       help="Training mode (quick/balanced/quality or legacy names)")
    parser.add_argument("--method", type=str, choices=["lora", "qlora"], help="Training method")
    
    # Training parameters
    parser.add_argument("--epochs", type=int, help="Number of epochs")
    parser.add_argument("--batch-size", type=int, help="Batch size")
    parser.add_argument("--learning-rate", type=float, help="Learning rate")
    parser.add_argument("--lora-r", type=int, help="LoRA rank")
    parser.add_argument("--lora-alpha", type=int, help="LoRA alpha")
    parser.add_argument("--target-modules", type=str, help="Comma-separated list of target modules")
    
    # Data parameters
    parser.add_argument("--max-examples", type=int, help="Maximum training examples")
    parser.add_argument("--validation-split", type=float, help="Validation split ratio")
    parser.add_argument("--max-seq-length", type=int, help="Maximum sequence length")
    parser.add_argument("--dataset-format", type=str, help="Dataset format")
    
    # Circular training
    parser.add_argument("--circular", action="store_true", help="Enable circular training")
    parser.add_argument("--max-circular-epochs", type=int, help="Maximum circular epochs")
    parser.add_argument("--circular-batch-multiplier", type=float, help="Batch size multiplier per cycle")
    
    # Resume training
    parser.add_argument("--resume", action="store_true", help="Resume from last checkpoint")
    parser.add_argument("--resume-from", type=str, help="Resume from specific checkpoint")
    
    # Auto-stop parameters
    parser.add_argument("--patience", type=int, help="Early stopping patience")
    parser.add_argument("--overfitting-threshold", type=float, help="Overfitting detection threshold")

    # Optimization
    parser.add_argument("--optimizer", type=str, help="Optimizer to use (e.g., adamw_torch, adamw_bnb_8bit)")
    
    # Hardware parameters
    parser.add_argument("--device", type=str, help="Device (auto/cuda/cpu)")
    parser.add_argument("--dtype", type=str, help="Data type (auto/float16/bfloat16/float32)")
    parser.add_argument("--use-8bit", action="store_true", help="Use 8-bit quantization")
    parser.add_argument("--use-4bit", action="store_true", help="Use 4-bit quantization")
    
    # Other parameters
    parser.add_argument("--test-prompt", type=str, help="Test prompt after training")
    parser.add_argument("--skip-test", action="store_true", help="Skip testing after training")
    parser.add_argument("--seed", type=int, help="Random seed")
    parser.add_argument("--force-epochs", action="store_true", help="Force training for specified epochs")
    
    args = parser.parse_args()
    
    # Load from config file if provided
    config_data = {}
    if args.config:
        import yaml
        with open(args.config, 'r') as f:
            config_data = yaml.safe_load(f)
    
    # Create a dictionary of command-line arguments that were explicitly set
    # We filter out arguments with value None or False (for action='store_true')
    cli_args = {k: v for k, v in vars(args).items() if v is not None and v is not False and k != 'config'}

    # Merge configurations: command-line arguments override config file
    final_config = {**config_data, **cli_args}

    # Check for required 'data' argument
    if 'data' not in final_config or not final_config['data']:
        raise ValueError("The --data argument is required, either in the config file or on the command line.")

    # Set random seed
    torch.manual_seed(final_config.get('seed', 42))
    np.random.seed(final_config.get('seed', 42))
    
    # Load model info
    model_path = final_config.get('model_path', './model')
    model_info_path = Path(model_path) / "model_info.json"
    if not model_info_path.exists():
        model_info_path = Path("model_info.json") # Check current dir as fallback
        if not model_info_path.exists():
            raise FileNotFoundError(f"model_info.json not found in {model_path} or current directory.")
    
    with open(model_info_path, 'r') as f:
        model_info = json.load(f)
    
    # Map new mode names to legacy names
    mode_mapping = {'quick': 'fast', 'balanced': 'medium', 'quality': 'slow'}
    current_mode = final_config.get('mode', 'balanced')
    if current_mode in mode_mapping:
        final_config['mode'] = mode_mapping[current_mode]
    
    # Create training config
    from training_config import TrainingConfig
    
    # Prepare kwargs for TrainingConfig
    config_kwargs = final_config.copy()
    config_kwargs['training_mode'] = final_config.get('mode', 'balanced')
    config_kwargs['num_train_epochs'] = final_config.get('epochs')
    config_kwargs['use_4bit'] = final_config.get('use_4bit') or final_config.get('method') == 'qlora'
    config_kwargs['model_id'] = model_info.get('model_id', 'unknown')

    if 'target_modules' in config_kwargs and isinstance(config_kwargs['target_modules'], str):
        config_kwargs['target_modules'] = [m.strip() for m in config_kwargs['target_modules'].split(',')]

    if final_config.get('resume') and not final_config.get('resume_from'):
        checkpoint_dir = Path(final_config.get('output', './lora')) / "checkpoints"
        if checkpoint_dir.exists():
            checkpoints = sorted(checkpoint_dir.glob("checkpoint-*"))
            if checkpoints:
                config_kwargs['resume_from_checkpoint'] = str(checkpoints[-1])

    training_config = TrainingConfig.from_model_info(model_info, **config_kwargs)

    # Explicitly set optimizer if provided via CLI to satisfy static analysis
    if 'optimizer' in cli_args:
        training_config.optimizer = cli_args['optimizer']
    
    # Print configuration
    print("\n" + "="*60)
    print("🎯 TRAINING CONFIGURATION")
    print("="*60)
    print(training_config.get_training_description())
    if args.force_epochs:
        print("\n🔒 FORCE EPOCHS MODE ENABLED")
        print("   Training will continue for full epochs regardless of:")
        print("   - Overfitting detection")
        print("   - Early stopping")
        print("   - Perfect loss")
    print("="*60 + "\n")
    
    # Load model and tokenizer
    print("📚 Loading model and tokenizer...")
    model, tokenizer, handler, training_params = load_model_and_tokenizer(
        args.model_path, asdict(training_config), model_info
    )
    
    # Setup LoRA
    print("🔧 Setting up LoRA...")
    model = setup_lora(model, asdict(training_config), training_params)
    
    # Load dataset
    print("📊 Loading dataset...")
    train_dataset, val_dataset = load_dataset(
        args.data, asdict(training_config), tokenizer, training_params
    )
    print(f"  Training examples: {len(train_dataset)}")
    if val_dataset:
        print(f"  Validation examples: {len(val_dataset)}")
    
    # Update config with actual dataset size for better eval_steps calculation
    training_config.update_with_dataset_size(len(train_dataset))
    print(f"  Evaluation frequency: every {training_config.eval_steps} steps")
    
    # Create output directory
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Start TensorBoard
    tb_process = None
    if 'tensorboard' in training_config.report_to:
        try:
            # Kill any existing TensorBoard process first
            subprocess.run(['pkill', '-f', 'tensorboard'], capture_output=True)
            time.sleep(1)  # Give it time to shut down
            
            tb_process = subprocess.Popen([
                'tensorboard',
                '--logdir', str(output_dir / "logs"),
                '--port', '6006',
                '--bind_all'
            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            print(f"📊 TensorBoard started at http://localhost:6006")
            print(f"   Monitoring logs at: {output_dir / 'logs'}")
        except Exception as e:
            logger.warning(f"Failed to start TensorBoard: {e}")
    
    try:
        # Create and run trainer
        trainer = SmartTrainer(
            model, tokenizer, train_dataset, val_dataset,
            asdict(training_config), output_dir, training_params
        )
        trainer.train()
        
        # Test model
        if not args.skip_test:
            test_model(args.model_path, args.output, args.test_prompt, training_params, 
                      asdict(training_config), args.data)
        
        print("\n✅ Training completed successfully!")
        print("\n📁 Output structure:")
        print(f"  {args.output}/")
        print(f"    ├── adapter_model.safetensors  # ← This is loaded when using the model")
        print(f"    ├── adapter_config.json")
        print(f"    ├── training_history.png")
        print(f"    └── checkpoints/               # ← Saved for safety, not used at inference")
        print(f"        ├── checkpoint-10/")
        print(f"        ├── checkpoint-20/")
        print(f"        └── ...")
        print("\nTo use a specific checkpoint instead of final:")
        print(f"  cp {args.output}/checkpoints/checkpoint-XXX/adapter_model.* {args.output}/")
        
    finally:
        # Cleanup
        if tb_process:
            tb_process.terminate()


if __name__ == "__main__":
    main()